{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEAXWod_uGX4"
   },
   "source": [
    "# NYPD Motor Vehicle Collision Analysis & Prediction <a id='Top'></a>\n",
    "\n",
    "**Student ID**: 100675726 \n",
    "\n",
    "**Student Name**: Marina Aslanidou\n",
    "\n",
    "**Module**: Independent Studies (6CC995)  \n",
    "\n",
    "**Project Title**: Predicting Fatal Crash Outcomes Using Machine Learning\n",
    "\n",
    "**Supervisor**: Mr. Ioannis Tsioulis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33bRnvcHuGX7"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This project analyses motor vehicle crashes in New York City through data from the NYC Open Data portal.\n",
    "The objective is to reveal the major patterns, contributing causes, and high-risk areas, and, through the use of machine learning models, determine the chances of deadly crashes.\n",
    "\n",
    "Key components of this study include:\n",
    "\n",
    "- Thorough data cleaning and preprocessing\n",
    "- Exploratory data analysis (EDA) throughout time, boroughs, and vehicle types \n",
    "- Statistical hypothesis testing (ANOVA, Chi-Square, and T-Test) \n",
    "- Predictive Modeling with Logistic Regression, Random Forest, and Multi-Layer Perceptron (MLP)\n",
    "- Visualizations such as heatmaps, correlation matrices, and ROC curves\n",
    "- Feature Importance Analysis and Explainability through Grad-CAM (for deep learning models) \n",
    "\n",
    "This notebook is organized in the style of an end-to-end data science workflow aimed at informing public safety activities through data-based insights.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Summary\n",
    "\n",
    "The dataset employed is the **\"Motor Vehicle Collisions â€“ Crashes\"** dataset on NYC Open Data.\n",
    "It consists of in-depth reports on crashes involving motor vehicles, including location, time, type of vehicle, contributing factor, and result.\n",
    "\n",
    "- **Data coverage:** 2012 onwards (updated on a daily basis)\n",
    "- **Size:** More than 1.6 million rows and 29 columns NYPD Motor Vehicle Collisions â€“ Crashes.\n",
    "- **Source:** [NYPD Motor Vehicle Collisions â€“ Crashes](https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions-Crashes/h9gi-nx95)\n",
    "\n",
    "This notebook goes through data import, exploration, transformation, modeling, and evaluation in its efforts to construct actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvdJXTFKuGX9"
   },
   "source": [
    "## Data Loading and Setup for Libraries <a id='Importing'></a>\n",
    "\n",
    "This part imports the Python libraries used for data processing, visualization, and statistical modeling.\n",
    "\n",
    "The **NYPD Motor Vehicle Collisions** dataset is loaded directly from the [NYC Open Data portal](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95), ensuring access to the most recent version of the data. A row limit of two million is used to future-proof the analysis, as the dataset continues to grow.\n",
    "\n",
    "> **Note on Reproducibility:**  \n",
    "> Because this dataset is updated daily, results (such as counts or model outputs) may vary slightly over time.  \n",
    "> To see the exact results generated during this analysis, please refer to the static PDF version of this notebook:  \n",
    "> [View the PDF snapshot with fixed results](https://mcclass-my.sharepoint.com/:b:/g/personal/mari_aslanidou_mc-class_gr/EUDck2apwcRJtWPk-d5uajwBtjB3q1zB5k8AP6TSGOFq3A?e=gCQxBK)\n",
    "\n",
    "> `low_memory=False` ensures pandas infers column types properly and avoids mixed-type warnings during import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FZx6gBWuGX-"
   },
   "outputs": [],
   "source": [
    "# imports numpy for numerical operations and arrays\n",
    "import numpy as np\n",
    "\n",
    "# imports pandas for working with dataframes and csv files\n",
    "import pandas as pd\n",
    "\n",
    "# imports datetime for handling date and time objects\n",
    "import datetime as dt\n",
    "\n",
    "# imports sys for accessing system-specific parameters and functions\n",
    "import sys\n",
    "\n",
    "# imports display and HTML tools from IPython to control how dataframes are shown\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxpm8DuduGX_"
   },
   "outputs": [],
   "source": [
    "# imports pandas for data handling\n",
    "import pandas as pd\n",
    "\n",
    "# loads the dataset from NYC Open Data portal (live version)\n",
    "url = \"https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD\"\n",
    "\n",
    "try:\n",
    "    # reads the dataset into a dataframe and disables low memory warning\n",
    "    datanyc = pd.read_csv(url, low_memory=False)\n",
    "    \n",
    "    # prints dataset shape for confirmation\n",
    "    print(f\" Data loaded successfully with {datanyc.shape[0]:,} rows and {datanyc.shape[1]} columns.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # error message and fallback if data fails to load\n",
    "    print(\" Failed to load dataset from URL. Please check your connection or use a local snapshot.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows general information about the dataframe including column types and non null values\n",
    "datanyc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_cleaning = len(datanyc)\n",
    "print(\"Rows before cleaning:\", before_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the ip address of the specified host used to confirm network or data source connection\n",
    "import socket\n",
    "socket.gethostbyname('data.cityofnewyork.us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcRd4AOmuGX_"
   },
   "source": [
    "And let's pull up the data dictionary supplied by the Open Data website for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2t9QX3auGYA",
    "outputId": "5b143c47-ec46-4611-9e2a-ce27acdb0cab"
   },
   "outputs": [],
   "source": [
    "# reads an excel file from a url containing the data dictionary for the dataset\n",
    "# selects the 'Column Info' sheet and uses the second row as the header\n",
    "data_dict = pd.read_excel(\n",
    "    \"https://data.cityofnewyork.us/api/views/h9gi-nx95/files/2e58023a-21a6-4c76-b9e8-0101bf7509ca?download=true&filename=MVCollisionsDataDictionary.xlsx\",\n",
    "    sheet_name='Column Info',\n",
    "    header=1\n",
    ")\n",
    "\n",
    "# shows the first five rows of the data dictionary to understand column meanings\n",
    "data_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39k45zLhuGYB"
   },
   "source": [
    "<br><div style=\"text-align: right\">[Begining of the page](#Top)</div>\n",
    "\n",
    "### Understanding the Data <a id='Understanding'></a>\n",
    "Let's look at the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNW_kQcIuGYB",
    "outputId": "42b15768-ed81-4550-ec85-4832a9b42691"
   },
   "outputs": [],
   "source": [
    "# sets display option to show all columns when displaying a dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# sets display option to limit output to 200 rows to avoid accidental overflow\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# shows the first five rows of the dataset\n",
    "datanyc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Pk0621juGYC"
   },
   "source": [
    "... And get overall information about the contents of the data. <a id='column_contents'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMuYs6GNuGYC",
    "outputId": "011d04cc-3fd3-4e44-b82a-de732132980f"
   },
   "outputs": [],
   "source": [
    "# increases the limit for rows shown in dataframe info to ensure full structure is displayed\n",
    "pd.options.display.max_info_rows = 2000000\n",
    "\n",
    "# displays dataset information including column types and non null counts\n",
    "datanyc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpwRE8Z5uGYD"
   },
   "source": [
    "Each column should contain approxamitely 1.6 million values, though some columns have considerably fewer entries. Let's find the percentage of the missing values and see which columns have the most amount of missing values. To do so we will get a mean of the missing values and then round it to the second decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNNitDk6uGYD",
    "outputId": "8ce3fe7d-8c70-4637-c276-13e330b0d833"
   },
   "outputs": [],
   "source": [
    "# sets the display option to show up to 29 columns at once\n",
    "pd.set_option('display.max_columns', 29)\n",
    "\n",
    "# calculates the percentage of missing values in each column rounded to four decimals\n",
    "datanyc.isnull().mean().round(4) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5L1w4MvuGYD"
   },
   "source": [
    "## Missing Data Overview\n",
    "\n",
    "Prior to cleaning the data, we need to know where the missing values occur and in what quantities.\n",
    "\n",
    "The following table displays the percentage missing for selected columns. Certain features like `contributing_factor_vehicle_3` or `vehicle_type_code_5` frequently appear blank, perhaps owing to their optional nature (i.e., not all crashes include multiple vehicles, contributing factors, etc.).\n",
    "\n",
    "We also visualize missing data in the form of a heatmap in order to detect gaps in important geographic and temporal fields like `BOROUGH`, `LATITUDE`, and `ZIP CODE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualize missing data before cleaning\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(datanyc[['CRASH DATE', 'BOROUGH', 'LATITUDE', 'LONGITUDE', 'ZIP CODE']].isnull(), \n",
    "            cbar=False, cmap='magma')\n",
    "plt.title(\"Heatmap of Missing Values in Collision Dataset (Before Cleaning)\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Records\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Tq_NOpCuGYE",
    "outputId": "f152214c-e071-462a-86fe-527ea2b8efb4"
   },
   "outputs": [],
   "source": [
    "# provides summary statistics for all numerical columns in the dataset\n",
    "datanyc.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y7l46jjuGYE"
   },
   "source": [
    "While averages and standard deviations don't give much information about latitude and longitude, they can help confirm the data structure.\n",
    "Since these fields have fewer entries than expected, it suggests that there is incomplete location information. Which leads to further investigation.\n",
    "\n",
    "This block will look at some of the empty `latitude` column using the `isnull` function. <a id='empty'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfxH2hbjuGYE",
    "outputId": "b2d3c59c-f480-491c-a738-d9852751eada"
   },
   "outputs": [],
   "source": [
    "# filters and displays the first 20 rows where the latitude value is missing\n",
    "datanyc[datanyc['LATITUDE'].isnull()].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJflRbRuuGYE"
   },
   "source": [
    "The first 20 rows with missing latitude values were inspected. While these rows lack geospatial coordinates, they shouldn't be deleted, as the injury/fatality data may be useful. For location-based analysis, these rows may be dropped however, including them could be justified in non-spatial statistical evaluations. \n",
    "\n",
    "For now, we'll pivot to take a closer look at vehicle types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehkzIhGOuGYF",
    "outputId": "60bdeb48-6820-4511-82a3-f00336347a98"
   },
   "outputs": [],
   "source": [
    "# counts the 20 most common values in the vehicle type code 1 column\n",
    "datanyc['VEHICLE TYPE CODE 1'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-y1uPa6uGYF",
    "outputId": "87917481-e99c-4590-8460-e1e344624441"
   },
   "outputs": [],
   "source": [
    "# counts the 20 most frequent values in the vehicle type code 3 column\n",
    "datanyc['VEHICLE TYPE CODE 3'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWKFwXLuuGYG"
   },
   "source": [
    "The frequency distributions for vehicle type codes 1 and 3 suggest that free text input, as opposed to controlled selection, was most frequently used to capture these fields.  This has led to inconsistent naming and duplicate values (e.g., \"Taxi\" vs \"TAXI\").\n",
    "\n",
    " Significantly fewer non-null entries are seen in columns outside of `VEHICLE TYPE CODE 1`, which may indicate secondary or tertiary vehicles involved in multi-car accidents.  This is a plausible assumption, even though the data dictionary does not specifically affirm it.\n",
    "\n",
    " `VEHICLE TYPE CODE 1` will continue to be the main focus of the `vehicle-type-related` study due to data sparsity in codes 2â€“5.\n",
    "\n",
    " In the following step, the dataset is prepared for confident analysis, missing values are addressed, duplicates are eliminated, and columns are renamed for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvJeISxDuGYG"
   },
   "source": [
    "## Transforming the Data <a id='Transforming'></a>\n",
    "\n",
    "## Data Cleaning and Transformation\n",
    "\n",
    "Before building predictive models, the dataset must be cleaned and standardized. This section outlines key steps taken to prepare the data for analysis.\n",
    "\n",
    "### What Needs Attention\n",
    "\n",
    "#### ðŸ”¹ Dropping Columns (#Drop)\n",
    "- Columns with more than 70% missing values (e.g., `VEHICLE TYPE CODE 4`, `CONTRIBUTING FACTOR VEHICLE 5`) are removed.\n",
    "- Irrelevant columns, such as `COLLISION_ID`, `ON STREET NAME`, `OFF STREET NAME`, and `CROSS STREET NAME`, are also dropped.\n",
    "\n",
    "#### ðŸ”¹ Renaming & Cleaning (#Renaming)\n",
    "- Inconsistent text fields (e.g., \"TAXI\" vs \"Taxi\") will be standardized.\n",
    "- Columns with unclear names will be renamed for better readability.\n",
    "- Some missing values may be imputed or ignored depending on context.\n",
    "\n",
    "#### ðŸ”¹ Redundant Columns (#Redundant)\n",
    "- `LATITUDE` and `LONGITUDE` are also captured in the `LOCATION` column. While both are kept for now, `LOCATION` may be dropped in later steps.\n",
    "\n",
    "#### ðŸ”¹ Data Type Fixes (#data_type)\n",
    "- `CRASH DATE` and `CRASH TIME` are currently stored as strings and will be converted to datetime objects.\n",
    "- `ZIP CODE` will be treated as a string to preserve formatting (leading zeroes, etc.).\n",
    "\n",
    "#### ðŸ”¹ Categorizing Crash Season (#categorizing)\n",
    "To support seasonal analysis, a new feature will classify each crash into:\n",
    "- **Spring** (Marchâ€“May)\n",
    "- **Summer** (Juneâ€“August)\n",
    "- **Fall** (Septemberâ€“November)\n",
    "- **Winter** (Decemberâ€“February)\n",
    "\n",
    "---\n",
    "\n",
    "### Dropping Columns\n",
    "\n",
    "Columns with excessive missing data or no analytical value are removed below.\n",
    "### What needs attention\n",
    "####  [Dropping Columns](#Drop)\n",
    "* Some columns (such as `VEHICLE TYPE CODE 4`, `CONTRIBUTING FACTOR VEHICLE 5`) are nearly empty, therefore, they will be removed.\n",
    "* Some columns will not be used (e.g. `collision_id`, `on_street_name`, `off_street_name`, `cross_street_name`) so they can be dropped completely.\n",
    "\n",
    "<br><div style=\"text-align: right\">[Begining of the page](#Top)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lf2oedZJuGYG"
   },
   "outputs": [],
   "source": [
    "# drops columns that have more than 70 percent missing values and copies the result into a new dataframe\n",
    "clean_nyc = datanyc.dropna(thresh=(0.30 * datanyc.shape[0]), axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4fcB97PuGYH"
   },
   "outputs": [],
   "source": [
    "# removes specific columns that are not needed for the analysis and ignores errors if any column is missing\n",
    "clean_nyc.drop(columns=[\"COLLISION_ID\", \"ON STREET NAME\", \"OFF STREET NAME\"], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V79DwKz3uGYH"
   },
   "source": [
    "The updated stucture of 'clean_nyc' is displayed below, as far as data types and number of columns (and values in those columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTV2kgMsuGYH",
    "outputId": "af7a8ee6-d11a-4db5-c977-fd99dba384f8"
   },
   "outputs": [],
   "source": [
    "# displays updated information about the dataframe after column removal\n",
    "clean_nyc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Dsid5UQuGYH"
   },
   "source": [
    "And what about the percentage of the missing values now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gmk1feuFuGYI",
    "outputId": "ebd7d256-07d7-4616-d5dd-b0c7f1f9a20a"
   },
   "outputs": [],
   "source": [
    "# sets the display limit to show up to 29 columns at once\n",
    "pd.set_option('display.max_columns', 29)\n",
    "\n",
    "# calculates the percentage of missing values in each remaining column\n",
    "clean_nyc.isnull().mean().round(4) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N72zpTQDuGYI"
   },
   "source": [
    "So far, so good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUDphOA8uGYI"
   },
   "source": [
    "<br><div style=\"text-align: right\">[Transformation Index](#Transforming)</div>\n",
    "## Correcting Misspellings and Renaming  <a id='Renaming'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7arU53PnuGYI"
   },
   "source": [
    "To standardize the dataset using the rename function, the column names will be renamed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HQmDn1ZuGYI"
   },
   "outputs": [],
   "source": [
    "# corrects column names by adding underscores for consistency with other variable names\n",
    "clean_nyc.rename(columns={\n",
    "    'vehicle_type_code1': 'vehicle_type_code_1',\n",
    "    'vehicle_type_code2': 'vehicle_type_code_2'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJsf-4hjuGYJ"
   },
   "source": [
    "Now looking closer at `VEHICLE TYPE CODE 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8ZsIoBguGYJ",
    "outputId": "be0f0551-e1ea-47e7-e9b3-2b0bcb095492"
   },
   "outputs": [],
   "source": [
    "# shows the 40 most common values in the vehicle type code 1 column\n",
    "clean_nyc['VEHICLE TYPE CODE 1'].value_counts().head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdBR2B2yuGYJ"
   },
   "source": [
    "It looks like there are misspellings and duplicates. Combining some of the obvious misspellings could be a solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O46z7JdsuGYJ",
    "outputId": "d5a6eaf7-739b-47b1-a7fb-c83ac208f60e"
   },
   "outputs": [],
   "source": [
    "# creates a dictionary to group and standardize different vehicle type labels under consistent categories\n",
    "replace_dict = {\n",
    "    'SPORT UTILITY / STATION WAGON': 'SUV',\n",
    "    'Station Wagon/Sport Utility Vehicle': 'SUV',\n",
    "    'TAXI': 'taxi',\n",
    "    'Bike': 'BICYCLE',\n",
    "    'VAN': 'Van',\n",
    "    'Motorscooter': 'SCOOTER',\n",
    "    'Moped': 'SCOOTER',\n",
    "    'van': 'Van',\n",
    "    'MOTORCYCLE': 'Motorcycle',\n",
    "    'AMBULANCE': 'Ambulance',\n",
    "    'Refrigerated Van': 'Van',\n",
    "    'PICK-UP TRUCK': 'Pick-up Truck',\n",
    "    'Motorbike': 'Motorcycle',\n",
    "    'AMBUL': 'Ambulance',\n",
    "    'CAB': 'taxi',\n",
    "    'Cab': 'taxi',\n",
    "    'VAN T': 'Van',\n",
    "    'VAN/T': 'Van',\n",
    "    'van t': 'Van',\n",
    "    'Ambul': 'Ambulance',\n",
    "    'AMB': 'Ambulance',\n",
    "    'Ambu': 'Ambulance',\n",
    "    'ambul': 'Ambulance',\n",
    "    'Fire': 'FIRE TRUCK',\n",
    "    'fire': 'FIRE TRUCK',\n",
    "    'FIRE': 'FIRE TRUCK',\n",
    "    'FIRET': 'FIRE TRUCK',\n",
    "    'FDNY': 'FIRE TRUCK',\n",
    "    'Other': 'Unknown',\n",
    "    'BUS': 'Bus',\n",
    "    'Box T': 'Box Truck',\n",
    "    'GARBA': 'Garbage or Refuse',\n",
    "    'Taxi': 'taxy',\n",
    "    'taxy': 'taxi',\n",
    "    'AM': 'Ambulance',\n",
    "    'VN': 'Van',\n",
    "    'CONV': 'Convertible',\n",
    "    'Garbage or Refuse': 'Dump',\n",
    "    'OTHER': 'UNKNOWN'\n",
    "}\n",
    "\n",
    "# replaces inconsistent vehicle type values in the dataset using the dictionary\n",
    "clean_nyc['VEHICLE TYPE CODE 1'] = clean_nyc['VEHICLE TYPE CODE 1'].replace(replace_dict)\n",
    "\n",
    "# displays the 50 most frequent cleaned vehicle type values\n",
    "clean_nyc['VEHICLE TYPE CODE 1'].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1V3qJohPuGYK"
   },
   "source": [
    "Even in one column, the amount of variation there is can be seen. It would've been better if whoever created this dataset turns this from a \"fill in the blank\" text field to a select field from a predetermined list, to get better data fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJyQX5aQuGYb"
   },
   "source": [
    "Now looking at `CONTRIBUTING FACTOR VEHICLE 1` and `CONTRIBUTING FACTOR VEHICLE 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5lJNNSjuGYb",
    "outputId": "ea5bf546-c407-4373-f77c-954012400cf7"
   },
   "outputs": [],
   "source": [
    "# displays all unique values found in the contributing factor vehicle 1 column\n",
    "clean_nyc['CONTRIBUTING FACTOR VEHICLE 1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gJrLpv9uGYc",
    "outputId": "e4fccac6-31ac-4c4b-9f4a-db5d20dc8c14"
   },
   "outputs": [],
   "source": [
    "# lists all unique values in the contributing factor vehicle 2 column to inspect secondary causes\n",
    "clean_nyc['CONTRIBUTING FACTOR VEHICLE 2'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRVERPU8uGYc"
   },
   "source": [
    "They have similar factors and there seem to be a lot of unique values. For simplicity and clarity, this project will focus exclusively on `CONTRIBUTING FACTOR VEHICLE 1`.\n",
    "\n",
    "Categorisation will be performed in `CONTRIBUTING FACTOR VEHICLE 1` to make the analysis a little bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QO_Ii7L-uGYd",
    "outputId": "ea799f38-7730-418d-ddf3-dc0b6214589f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shows all unique values from the original column name with uppercase and spaces\n",
    "clean_nyc['CONTRIBUTING FACTOR VEHICLE 1'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPwei0F7uGYd"
   },
   "source": [
    "The entries `'80'` and `'1'` do not convey meaningful information and are removed from the dataset.  \n",
    "Similarly, rows containing `'Unspecified'` or missing values are excluded to ensure a more informative and accurate analysis.  \n",
    "This also enhances the clarity of subsequent visualisations and statistical summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGE_auW-uGYd"
   },
   "outputs": [],
   "source": [
    "# simplifies and groups related contributing factors under broader categories for analysis\n",
    "clean_nyc['CONTRIBUTING FACTOR VEHICLE 1'] = clean_nyc['CONTRIBUTING FACTOR VEHICLE 1'].replace({\n",
    "    'Backing Unsafely': 'Traffic Recklessness',\n",
    "    'Unsafe Speed': 'Traffic Recklessness',\n",
    "    'Passing or Lane Usage Improper': 'Traffic Recklessness',\n",
    "    'Turning Improperly': 'Traffic Recklessness',\n",
    "    'Following Too Closely': 'Traffic Recklessness',\n",
    "    'Passing Too Closely': 'Traffic Recklessness',\n",
    "    'Outside Car Distraction': 'Traffic Recklessness',\n",
    "    'Steering Failure': 'Traffic Recklessness',\n",
    "    'Reaction to Uninvolved Vehicle': 'Traffic Recklessness',\n",
    "    'Traffic Control Disregarded': 'Traffic Recklessness',\n",
    "    'Failure to Yield Right-of-Way': 'Traffic Recklessness',\n",
    "    'Aggressive Driving/Road Rage': 'Traffic Recklessness',\n",
    "    'Unsafe Lane Changing': 'Traffic Recklessness',\n",
    "    'Driver Inexperience': 'Traffic Recklessness',\n",
    "\n",
    "    'Passenger Distraction': 'Driver Inattention/Distraction',\n",
    "    'Failure to Keep Right': 'Driver Inattention/Distraction',\n",
    "    'Eating or Drinking': 'Driver Inattention/Distraction',\n",
    "    'Animals Action': 'Driver Inattention/Distraction',\n",
    "    'Using On Board Navigation Device': 'Driver Inattention/Distraction',\n",
    "    'Reaction to Other Uninvolved Vehicle': 'Driver Inattention/Distraction',\n",
    "    'Cell Phone (hands-free)': 'Driver Inattention/Distraction',\n",
    "    'Cell Phone (hand-Held)': 'Driver Inattention/Distraction',\n",
    "    'Cell Phone (hand-held)': 'Driver Inattention/Distraction',\n",
    "    'Other Electronic Device': 'Driver Inattention/Distraction',\n",
    "    'Texting': 'Driver Inattention/Distraction',\n",
    "    'Listening/Using Headphones': 'Driver Inattention/Distraction',\n",
    "    'Fatigued/Drowsy': 'Driver Inattention/Distraction',\n",
    "    'Fell Asleep': 'Driver Inattention/Distraction',\n",
    "\n",
    "    'Brakes Defective': 'Car Defects',\n",
    "    'Tinted Windows': 'Car Defects',\n",
    "    'Tire Failure/Inadequate': 'Car Defects',\n",
    "    'Tow Hitch Defective': 'Car Defects',\n",
    "    'Headlights Defective': 'Car Defects',\n",
    "    'Accelerator Defective': 'Car Defects',\n",
    "    'Windshield Inadequate': 'Car Defects',\n",
    "    'Driverless/Runaway Vehicle': 'Car Defects',\n",
    "    'Oversized Vehicle': 'Car Defects',\n",
    "\n",
    "    'Glare': 'Road Defects',\n",
    "    'Tinted Windows': 'Road Defects',\n",
    "    'Lane Marking Improper/Inadequate': 'Road Defects',\n",
    "    'View Obstructed/Limited': 'Road Defects',\n",
    "    'Pavement Defective': 'Road Defects',\n",
    "    'Other Lighting Defects': 'Road Defects',\n",
    "    'Obstruction/Debris': 'Road Defects',\n",
    "    'Traffic Control Device Improper/Non-Working': 'Road Defects',\n",
    "    'Shoulders Defective/Improper': 'Road Defects',\n",
    "    'Pavement Slippery': 'Road Defects',\n",
    "\n",
    "    'Illnes': 'Illness',\n",
    "    'Lost Consciousness': 'Illness',\n",
    "    'Physical Disability': 'Illness',\n",
    "    'Prescription Medication': 'Illness',\n",
    "\n",
    "    'Drugs (illegal)': 'Drugs (Illegal)',\n",
    "    'Alcohol Involvement': 'Drugs (Illegal)',\n",
    "\n",
    "    'Pedestrian/Bicyclist/Other Pedestrian Error/Confusion': 'Outside Error',\n",
    "    'Vehicle Vandalism': 'Outside Error',\n",
    "    'Other Vehicular': 'Outside Error'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKQs4mvRuGYe",
    "outputId": "3d184042-a424-474f-e784-d8f8a7d73e8f"
   },
   "outputs": [],
   "source": [
    "# finds and drops rows where the contributing factor is incorrectly marked as 80\n",
    "nyc80 = clean_nyc[clean_nyc['CONTRIBUTING FACTOR VEHICLE 1'] == '80'].index\n",
    "clean_nyc.drop(nyc80, inplace=True)\n",
    "\n",
    "# finds and drops rows where the contributing factor is incorrectly marked as 1\n",
    "nyc1 = clean_nyc[clean_nyc['CONTRIBUTING FACTOR VEHICLE 1'] == '1'].index\n",
    "clean_nyc.drop(nyc1, inplace=True)\n",
    "\n",
    "# finds and drops rows where the contributing factor is marked as unspecified\n",
    "dropunspecified = clean_nyc[clean_nyc['CONTRIBUTING FACTOR VEHICLE 1'] == 'Unspecified'].index\n",
    "clean_nyc.drop(dropunspecified, inplace=True)\n",
    "\n",
    "# drops rows where the contributing factor column is completely empty\n",
    "clean_nyc.dropna(subset=['CONTRIBUTING FACTOR VEHICLE 1'], how='all', inplace=True)\n",
    "\n",
    "# checks the remaining unique values in the contributing factor column\n",
    "clean_nyc['CONTRIBUTING FACTOR VEHICLE 1'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-TCNnVauGYe"
   },
   "source": [
    "Next, some columns will be renambed to make things easier while analyzing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iE4zPJY6uGYe",
    "outputId": "19d9c14a-e19a-4bc2-f7ac-3daba0b73fcf"
   },
   "outputs": [],
   "source": [
    "# renames injury and fatality related columns to shorter and clearer names for readability\n",
    "clean_nyc.rename(columns={\n",
    "    'number_of_persons_injured': 'persons_injured',\n",
    "    'number_of_persons_killed': 'persons_killed',\n",
    "    'number_of_pedestrians_injured': 'pedestrians_injured',\n",
    "    'number_of_pedestrians_killed': 'pedestrians_killed',\n",
    "    'number_of_cyclist_injured': 'cyclist_injured',\n",
    "    'number_of_cyclist_killed': 'cyclist_killed',\n",
    "    'number_of_motorist_injured': 'motorist_injured',\n",
    "    'number_of_motorist_killed': 'motorist_killed'\n",
    "}, inplace=True)\n",
    "\n",
    "# displays the first five rows of the updated dataframe\n",
    "clean_nyc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kO5gG06CuGYf"
   },
   "source": [
    "Next, to ensure consistency in categorical variables, all text values will be converted to lowercase.  \n",
    "This standardisation simplifies string comparisons and improves the reliability of grouping operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPbSwze1uGYg",
    "outputId": "2fa97d2e-56e1-4239-b57d-c2750df5edaa"
   },
   "outputs": [],
   "source": [
    "# creates a copy of the cleaned dataframe to work on separately\n",
    "clean_nyc1 = clean_nyc.copy()\n",
    "\n",
    "# converts all column names to lowercase and replaces spaces with underscores for consistency\n",
    "clean_nyc1.columns = clean_nyc1.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# converts all text data in string columns to lowercase for uniformity\n",
    "for col in clean_nyc1.select_dtypes(include='object').columns:\n",
    "    clean_nyc1[col] = clean_nyc1[col].map(lambda s: s.lower() if isinstance(s, str) else s)\n",
    "\n",
    "# shows the first five rows of the updated dataframe\n",
    "clean_nyc1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Columns for Clarity\n",
    "To make the dataset easier to read and work with, the columns were renamed using more intuitive and simplified names. This improves code readability and supports cleaner analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for better readability\n",
    "clean_nyc1 = clean_nyc1.rename(columns={\n",
    "    \"crash_date\": \"date\",\n",
    "    \"crash_time\": \"time\",\n",
    "    \"borough\": \"borough\",\n",
    "    \"zip_code\": \"zip\",\n",
    "    \"latitude\": \"lat\",\n",
    "    \"longitude\": \"lon\",\n",
    "    \"location\": \"location\",\n",
    "    \"on_street_name\": \"street_on\",\n",
    "    \"cross_street_name\": \"street_cross\",\n",
    "    \"off_street_name\": \"street_off\",\n",
    "    \"number_of_persons_injured\": \"injured_persons\",\n",
    "    \"number_of_persons_killed\": \"killed_persons\",\n",
    "    \"number_of_pedestrians_injured\": \"injured_pedestrians\",\n",
    "    \"number_of_pedestrians_killed\": \"killed_pedestrians\",\n",
    "    \"number_of_cyclist_injured\": \"injured_cyclists\",\n",
    "    \"number_of_cyclist_killed\": \"killed_cyclists\",\n",
    "    \"number_of_motorist_injured\": \"injured_motorists\",\n",
    "    \"number_of_motorist_killed\": \"killed_motorists\",\n",
    "    \"contributing_factor_vehicle_1\": \"factor_1\",\n",
    "    \"contributing_factor_vehicle_2\": \"factor_2\",\n",
    "    \"contributing_factor_vehicle_3\": \"factor_3\",\n",
    "    \"contributing_factor_vehicle_4\": \"factor_4\",\n",
    "    \"contributing_factor_vehicle_5\": \"factor_5\",\n",
    "    \"collision_id\": \"collision_id\",\n",
    "    \"vehicle_type_code_1\": \"vehicle_type_1\",\n",
    "    \"vehicle_type_code_2\": \"vehicle_type_2\",\n",
    "    \"vehicle_type_code_3\": \"vehicle_type_3\",\n",
    "    \"vehicle_type_code_4\": \"vehicle_type_4\",\n",
    "    \"vehicle_type_code_5\": \"vehicle_type_5\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TepBAdfFuGYg"
   },
   "source": [
    "<br><div style=\"text-align: right\">[Transformation Index](#Transforming)</div>\n",
    "## Redundant Columns: 'latitude' and 'longitude' <a id='Redundant'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkr9w1NRuGYg"
   },
   "source": [
    "It is suspected that the `location` column is simply a concatenation of the `latitude` and `longitude` columns. To gather information about it, its possible to check the data dictionary as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtKE-1aMuGYh",
    "outputId": "d7811990-994e-4661-8c66-41449dd297bb"
   },
   "outputs": [],
   "source": [
    "# displays rows 5 to 7 from the data dictionary dataframe for reference\n",
    "data_dict[5:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVXq5VwRuGYh"
   },
   "source": [
    "The data dictionary describes `LOCATION` as a \"Latitude, Longitude pair\", suggesting it duplicates the `LATITUDE` and `LONGITUDE` columns.  \n",
    "Further verification is needed before deciding to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQPEq-SiuGYh",
    "outputId": "20de7dcf-26e0-4178-e3be-f5556e457f43"
   },
   "outputs": [],
   "source": [
    "# displays the first 30 rows of lat, lon, and location columns\n",
    "clean_nyc1[[\"lat\", \"lon\", \"location\"]].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8N2XJJ8uGYi"
   },
   "source": [
    "A visual inspection confirms that `LOCATION` replicates the structure of `LATITUDE` and `LONGITUDE`.  \n",
    "Before proceeding with column removal, a more comprehensive check is performed to ensure formatting consistency across all rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhNRPykQuGYi",
    "outputId": "0455dccf-aaa1-4901-e881-121b169ddb42"
   },
   "outputs": [],
   "source": [
    "# checks how many location values match a specific point format using a regular expression\n",
    "clean_nyc1['location'].str.match(r'POINT \\(-7\\d\\.\\d+ \\d{2}\\.\\d+\\)', na=False).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hx48D7WXuGYj"
   },
   "source": [
    "1,432,680 rows do not match the expected coordinate format in the `LOCATION` column â€” significantly more than anticipated.  \n",
    "A closer inspection is needed to understand this inconsistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8iWjoGfuGYj",
    "outputId": "1011d544-b13d-43fd-d39a-b3771531ef26"
   },
   "outputs": [],
   "source": [
    "# filters and displays rows where the location does not match the expected point format\n",
    "clean_nyc1[~clean_nyc1['location'].str.match(r'POINT \\(-7\\d\\.\\d+ \\d{2}\\.\\d+\\)', na=False)].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFKSwhm8uGYk"
   },
   "source": [
    "Many of the non-matching `location` values are missing (`NaN`), aligning with earlier observations about incomplete coordinate data.  \n",
    "Since `location` appears to be a duplicate of the `lat` and `lon` fields and contains inconsistent formatting, it is safe to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zldCMCBuGYk"
   },
   "outputs": [],
   "source": [
    "# removes the location column from the dataframe as it no longer matches the expected format\n",
    "clean_nyc1.drop(columns=\"location\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M_PDBvEuGYk"
   },
   "source": [
    "Below is the cleaned version after dropping `location`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FKp1peguGYk",
    "outputId": "a8b6551e-626e-4a1c-cc45-3e3ea1d05588"
   },
   "outputs": [],
   "source": [
    "# displays the first five rows of the cleaned dataframe to review changes\n",
    "clean_nyc1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDxtjlSeuGYl"
   },
   "source": [
    "<br><div style=\"text-align: right\">[Transformation Index](#Transforming)</div>\n",
    "## Data Type <a id='data_type'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UJwLJ1XuGYl"
   },
   "source": [
    "Now looking at the dates to make sure they are all in the same format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7K6vOqGuGYl",
    "outputId": "6f689829-e051-4e81-e4af-16d956800eec"
   },
   "outputs": [],
   "source": [
    "# displays the first five rows of the updated column names\n",
    "clean_nyc1[['date', 'time']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaKwdKiNuGYl"
   },
   "source": [
    "The `crash_date` column is in string format and needs to be converted to a proper datetime type for accurate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hAVY0jeuGYl",
    "outputId": "efc1e587-7293-48cc-afdf-1130a3b3ae19"
   },
   "outputs": [],
   "source": [
    "# converts the 'date' column to datetime format for easier manipulation and analysis\n",
    "clean_nyc1['date'] = pd.to_datetime(clean_nyc1['date'])\n",
    "\n",
    "# displays the first five rows to confirm the date format conversion\n",
    "clean_nyc1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ELOVexUuGYm"
   },
   "source": [
    "A column with only the hour should be created. It could be helpful when grouping the times and visualising the data. We will create a new column called `hour` in which there will that will have hours instead of hours and minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNFmgUHbuGYm",
    "outputId": "4159f27e-46ec-4b4e-b019-8eb5d22fda1b"
   },
   "outputs": [],
   "source": [
    "# converts the 'time' column to datetime format using the expected format HH:MM\n",
    "clean_nyc1['time'] = pd.to_datetime(clean_nyc1['time'], format='%H:%M', errors='coerce')\n",
    "\n",
    "# extracts the hour from the 'time' column for analysis\n",
    "clean_nyc1['hour'] = clean_nyc1['time'].dt.hour\n",
    "\n",
    "# displays the first five rows to confirm the changes\n",
    "clean_nyc1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITk9SQXxuGYn"
   },
   "source": [
    "The `zip` column was converted to string type to preserve formatting consistency, especially for ZIP codes with leading zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osA_Xe0VuGYn"
   },
   "outputs": [],
   "source": [
    "# converts the 'zip' column to string type for consistency\n",
    "clean_nyc1.loc[:, 'zip'] = clean_nyc1['zip'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Collisions by Year Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Convert 'CRASH DATE' to datetime\n",
    "datanyc['CRASH DATE'] = pd.to_datetime(datanyc['CRASH DATE'], errors='coerce')\n",
    "\n",
    "# Extract year\n",
    "datanyc['YEAR'] = datanyc['CRASH DATE'].dt.year\n",
    "\n",
    "# Group by year and count collisions\n",
    "yearly_counts = datanyc.groupby('YEAR').size()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "yearly_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title(\"Total Motor Vehicle Collisions per Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Collisions\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Crash Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing coordinates\n",
    "geo_data = datanyc.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"])\n",
    "\n",
    "# Optional: sample for performance\n",
    "geo_sample = geo_data.sample(n=1000, random_state=42)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(geo_sample[\"LONGITUDE\"], geo_sample[\"LATITUDE\"], alpha=0.5, s=10, color='crimson')\n",
    "plt.title(\"NYC Motor Vehicle Collision Locations\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monthly Collision Trend Line Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by month and count total collisions\n",
    "datanyc['CRASH DATE'] = pd.to_datetime(datanyc['CRASH DATE'], errors='coerce')\n",
    "monthly = datanyc.resample('M', on='CRASH DATE').size()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly.plot(color='darkgreen')\n",
    "plt.title(\"Monthly Trend of Motor Vehicle Collisions in NYC\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Collisions\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bar Chart of Collisions per Borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total collisions per borough\n",
    "borough_counts = datanyc['BOROUGH'].value_counts()\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "borough_counts.plot(kind='bar', color='royalblue')\n",
    "plt.title(\"Total Collisions by NYC Borough\")\n",
    "plt.xlabel(\"Borough\")\n",
    "plt.ylabel(\"Number of Collisions\")\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rozybl9NuGYr"
   },
   "source": [
    "<br><div style=\"text-align: right\">[Transformation Index](#Transforming)</div>\n",
    "## Categorizing<a id='categorizing'></a>: Making a Seasons Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPooHu4fuGYs"
   },
   "source": [
    "Next step would be adding a variable that shows the season in which a crash occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOXVvFuguGYs",
    "outputId": "ddeca269-6b13-4ca3-879c-340bf957dd48"
   },
   "outputs": [],
   "source": [
    "# extracts the month from the 'date' column\n",
    "clean_nyc1['date'].dt.month.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dmao290OuGYs"
   },
   "outputs": [],
   "source": [
    "# defines a function to categorize a date into a season based on the month\n",
    "def season(date):\n",
    "    if date.month in ([3, 4, 5]):\n",
    "        val = 'Spring'\n",
    "    elif date.month in ([6, 7, 8]):\n",
    "        val = 'Summer'\n",
    "    elif date.month in ([9, 10, 11]):\n",
    "        val = 'Autumn'\n",
    "    elif date.month in ([12, 1, 2]):\n",
    "        val = 'Winter'\n",
    "    else:\n",
    "        val = \"Unspecified\"\n",
    "    return val\n",
    "\n",
    "# applies the season function to the 'date' column and creates a new 'season' column\n",
    "clean_nyc1['season'] = clean_nyc1['date'].apply(season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uc2Uj-WuGYu",
    "outputId": "a2206c91-fb85-4af8-80be-cefdab544b9f"
   },
   "outputs": [],
   "source": [
    "# counts the occurrences of each season in the season column for summary\n",
    "clean_nyc1['season'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAhDIJpjuGYu"
   },
   "source": [
    "Before moving on to the analysis of the dataset, it would be preferred to take a quick look at the difference of the work made so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILHeUKjCuGYu",
    "outputId": "63bd548f-df95-458c-a7e3-46fce32c2fc9"
   },
   "outputs": [],
   "source": [
    "# shows the shape (rows, columns) of the original dataset\n",
    "datanyc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKqsqJQKuGYv",
    "outputId": "c57dc524-c263-4c76-d0a3-461b579e5bee"
   },
   "outputs": [],
   "source": [
    "# shows the shape (rows, columns) of the cleaned dataset to compare\n",
    "clean_nyc1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ox987ANnuGYv"
   },
   "source": [
    "The original data included over `2.1 million` rows and `29 columns`. After cleaning, it was reduced to `1.43 million rows` and `21 columns`.\n",
    "This significant reduction improves clarity and removes unnecessary information.\n",
    "Below is a preview of the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHUtMqTyuGYv",
    "outputId": "16a2410c-f1b8-4cb9-bc35-dfee7e7ce7cc"
   },
   "outputs": [],
   "source": [
    "# displays the first five rows of the cleaned dataset to check final changes\n",
    "clean_nyc1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "996P2o85uGYv",
    "outputId": "b43e7c58-b84c-4075-b366-73922858226f"
   },
   "outputs": [],
   "source": [
    "# displays general info about the cleaned dataset including column types and non-null counts\n",
    "clean_nyc1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8POslx3uGYw"
   },
   "source": [
    "The dataset has been cleaned, renamed, and reduced to 21 columns. It now includes standardized types and structured fields such as `injury counts`, `vehicle types`, and `contributing factors`.\n",
    "To save a local copy of the cleaned data, remove the `#` from the line below and run the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSkV_ezpuGYw"
   },
   "outputs": [],
   "source": [
    "# saves the cleaned dataset as a CSV file (this line is commented out, but can be used when ready)\n",
    "# clean_nyc1.to_csv(\"nyc_crash_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAgZp8FduGYx"
   },
   "source": [
    "<br><div style=\"text-align: right\">[Begining of the page](#Top)</div>\n",
    "\n",
    "# Analyzing the data <a id='Analyzing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuXgsbctuGYx"
   },
   "source": [
    "To see how many cyclists, pedestrians and motorists were killed in all of our crashes, first, a dataframe will be created that extracts the rows where fatalities occurred. Then the same process will be used to extract the different categories of people, as defined by the data dictionary, and create a new column which creates a cumulative sum of the number of people killed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ql8YLAzZuGYx",
    "outputId": "84e9d8c1-3b3b-47d1-eda0-10b7ba726c15"
   },
   "outputs": [],
   "source": [
    "# filters the dataset to get only rows where at least one person was killed\n",
    "lethal_crashes = clean_nyc1[clean_nyc1['killed_persons'] > 0]\n",
    "\n",
    "# displays the first five rows of the filtered dataset\n",
    "lethal_crashes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqo1V_CbuGYy"
   },
   "outputs": [],
   "source": [
    "# groups lethal crashes by date and borough, then sums the number of motorists killed\n",
    "killed_motorists = lethal_crashes.groupby(['date', 'borough'])['killed_motorists'].sum().reset_index()  # changed column name to 'killed_motorists'\n",
    "# calculates the cumulative sum of killed motorists over time\n",
    "killed_motorists['sum'] = killed_motorists['killed_motorists'].cumsum()  # adjusted to match column name\n",
    "\n",
    "# groups lethal crashes by date and borough, then sums the number of cyclists killed\n",
    "killed_cyclist = lethal_crashes.groupby(['date', 'borough'])['killed_cyclists'].sum().reset_index()  # changed column name to 'killed_cyclists'\n",
    "# calculates the cumulative sum of killed cyclists over time\n",
    "killed_cyclist['sum'] = killed_cyclist['killed_cyclists'].cumsum()  # adjusted to match column name\n",
    "\n",
    "# groups lethal crashes by date and borough, then sums the number of pedestrians killed\n",
    "killed_pedestrians = lethal_crashes.groupby(['date', 'borough'])['killed_pedestrians'].sum().reset_index()  # changed column name to 'killed_pedestrians'\n",
    "# calculates the cumulative sum of killed pedestrians over time\n",
    "killed_pedestrians['sum'] = killed_pedestrians['killed_pedestrians'].cumsum()  # adjusted to match column name\n",
    "\n",
    "# groups lethal crashes by date and borough, then sums the total number of persons killed\n",
    "killed_persons = lethal_crashes.groupby(['date', 'borough'])['killed_persons'].sum().reset_index()  # changed column name to 'killed_persons'\n",
    "# calculates the cumulative sum of killed persons over time\n",
    "killed_persons['sum'] = killed_persons['killed_persons'].cumsum()  # adjusted to match column name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWpXJIVWuGYy"
   },
   "source": [
    "We will now display the last line of each of these data sets to find out how many people were killed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_xDzcBYuGYy",
    "outputId": "ca70b317-b272-49d3-8411-b541c8633661"
   },
   "outputs": [],
   "source": [
    "# displays the last row of the killed_motorists dataframe to see the final cumulative total\n",
    "killed_motorists.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "670gy6jquGYy",
    "outputId": "8943e16f-5dd0-4e7b-9826-e49ceddf8a40"
   },
   "outputs": [],
   "source": [
    "# displays the last row of the killed_pedestrians dataframe to see the final cumulative total\n",
    "killed_pedestrians.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjVjxA3auGYz",
    "outputId": "5153c13f-36a7-4798-9989-e3a2cd3d61eb"
   },
   "outputs": [],
   "source": [
    "# displays the last row of the killed_cyclist dataframe to see the final cumulative total\n",
    "killed_cyclist.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQw8wW6quGYz",
    "outputId": "85815d36-0088-4602-950b-8ba61bfc7ac1"
   },
   "outputs": [],
   "source": [
    "# displays the last row of the killed_persons dataframe to see the final cumulative total\n",
    "killed_persons.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT6bPtqpuGYz"
   },
   "source": [
    "The last row of each summary dataset shows the final cumulative total of fatalities for each category.\n",
    "\n",
    "Based on the totals, pedestrian fatalities appear higher than those of motorists or cyclists. Note that these figures do not reflect population size or exposure rates.\n",
    "\n",
    "The summed values of killed pedestrians, cyclists, and motorists do not exactly match the total count of killed persons. This small discrepancy likely results from minor inconsistencies in data entry or reporting.\n",
    "\n",
    "Now checking for crashes that involved injuries. The dataset is filtered to keep only rows where at least one person was injured.\n",
    "\n",
    "The `borough` column is not needed for this part, so it can be ignored in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rmka0PtfuGYz",
    "outputId": "4d210e7f-b6c5-4484-d9fc-b299f54286bd"
   },
   "outputs": [],
   "source": [
    "# filters the dataset to get only rows where at least one person was injured\n",
    "crashes_with_injuries = clean_nyc1[clean_nyc1['injured_persons'] > 0]\n",
    "\n",
    "# displays the first five rows of the filtered dataset\n",
    "crashes_with_injuries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BTHoPLluGYz"
   },
   "outputs": [],
   "source": [
    "# groups crashes with injuries by date and sums the number of motorists injured\n",
    "injured_motorists = crashes_with_injuries.groupby('date')['injured_motorists'].sum().reset_index()\n",
    "# calculates the cumulative sum of injured motorists over time\n",
    "injured_motorists['sum'] = injured_motorists['injured_motorists'].cumsum()\n",
    "\n",
    "# groups crashes with injuries by date and sums the number of cyclists injured\n",
    "injured_cyclist = crashes_with_injuries.groupby('date')['injured_cyclists'].sum().reset_index()\n",
    "# calculates the cumulative sum of injured cyclists over time\n",
    "injured_cyclist['sum'] = injured_cyclist['injured_cyclists'].cumsum()\n",
    "\n",
    "# groups crashes with injuries by date and sums the number of pedestrians injured\n",
    "injured_pedestrians = crashes_with_injuries.groupby('date')['injured_pedestrians'].sum().reset_index()\n",
    "# calculates the cumulative sum of injured pedestrians over time\n",
    "injured_pedestrians['sum'] = injured_pedestrians['injured_pedestrians'].cumsum()\n",
    "\n",
    "# groups crashes with injuries by date and sums the total number of persons injured\n",
    "injured_persons = crashes_with_injuries.groupby('date')['injured_persons'].sum().reset_index()\n",
    "# calculates the cumulative sum of injured persons over time\n",
    "injured_persons['sum'] = injured_persons['injured_persons'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEQe9HP1uGYz",
    "outputId": "a4244c4c-5f9f-4b17-e6f0-f1524bed74ab"
   },
   "outputs": [],
   "source": [
    "# displays the last row of the injured_motorists dataframe to see the final cumulative total\n",
    "injured_motorists.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpvK28aauGY0",
    "outputId": "d0147e96-2eeb-400f-b2f7-5215257ed7da"
   },
   "outputs": [],
   "source": [
    "# displays the last row of the injured_pedestrians dataframe to see the final cumulative total\n",
    "injured_pedestrians.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TDyxLd0uGY0",
    "outputId": "73db62cf-9311-4177-9c73-61b803755826"
   },
   "outputs": [],
   "source": [
    "# displays the last row of the injured_cyclist dataframe to see the final cumulative total\n",
    "injured_cyclist.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3_3fDrluGY0",
    "outputId": "64bfc97f-72c3-4374-9199-b8da3ccd6ab1"
   },
   "outputs": [],
   "source": [
    "# displays the last row of the injured_persons dataframe to see the final cumulative total\n",
    "injured_persons.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF_ymj1BuGY0"
   },
   "source": [
    "Motorists appear to be injured about four times more often than pedestrians in crashes.  \n",
    "This trend is different from the fatality data, where pedestrians were more likely to be killed than motorists.\n",
    "\n",
    "This section checks which boroughs report the most crash incidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "671Oecq-uGY0",
    "outputId": "800cac52-b36e-4f54-84fc-fcfa3e9f4b3c"
   },
   "outputs": [],
   "source": [
    "# counts the number of occurrences of each borough in the dataset\n",
    "clean_nyc1['borough'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxnvMZuIuGY1"
   },
   "source": [
    "Brooklyn reports the highest number of crashes. The Bronx and Staten Island have notably fewer incidents.\n",
    "\n",
    "The next step is to identify the most common contributing factors to crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMiD5Y9IuGY1",
    "outputId": "66af7f28-8a67-4b80-9e77-62a912bf4025"
   },
   "outputs": [],
   "source": [
    "# counts the occurrences of each contributing factor in the factor_1 column\n",
    "clean_nyc1['factor_1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFUv7L7HuGY1"
   },
   "source": [
    "The leading causes of crashes are `traffic recklessness` and `driver inattention` or `distraction`.\n",
    "\n",
    "To analyze vehicle types involved in crashes caused by driver inattention or distraction, a new dataset is created by filtering for those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p09ahU3FuGY1",
    "outputId": "3a831cdb-590f-4a48-d4d0-1af2dd24914a"
   },
   "outputs": [],
   "source": [
    "# filters the dataset to get only rows where driver inattention/distraction was the contributing factor\n",
    "contributing_factor_inattention = clean_nyc1[clean_nyc1['factor_1'] == \"driver inattention/distraction\"].copy()\n",
    "\n",
    "# displays the first five rows of the filtered dataset\n",
    "contributing_factor_inattention.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4_aqSzwuGY1"
   },
   "source": [
    "Now we can see which vehicles have the 'Driver Inattention' factor most frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42vz8BEGuGY1",
    "outputId": "e9deaca0-60c4-47df-9867-677c5d6f8485"
   },
   "outputs": [],
   "source": [
    "# counts the 30 most common vehicle types where driver inattention/distraction was the contributing factor\n",
    "contributing_factor_inattention['vehicle_type_1'].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAaicdehuGY2"
   },
   "source": [
    "`SUVs` and `sedans` are the most frequently involved vehicle types in crashes caused by driver inattention or distraction. `Passenger vehicles` follow behind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8slyicXjuGY2"
   },
   "source": [
    "# Statistical Analysis <a id='statistical_analysis'></a>\n",
    "The analysis uncovered a lot of interesting points, but at a glance, it's hard to tell whether this was simply due to chance. Was it random variation that revealed these points of interest, or are the differences actually significant? To find out, some statistical analyses will be performed:\n",
    "\n",
    "   - [ANOVA](#anova)\n",
    "   - [2 Sample T-Test](#2ttest)\n",
    "   - [Chi-Square](#chi-sq)\n",
    "<br><div style=\"text-align: right\">[Begining of the page](#Top)</div>\n",
    "## ANOVA <a id='anova'></a>\n",
    "\n",
    "The analysis begins by examining whether there is a difference between the number of deaths by borough for persons, pedestrians, motorists, and cyclists. A one-way ANOVA test will be used since there are more than two boroughs (more than two nominal variables).\n",
    "\n",
    "First, the `persons_killed` column will be cleaned by dropping NaN values to prepare for the ANOVA test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Data Availability for Each Borough\n",
    "Before performing the ANOVA test, it's important to check how many valid entries exist for the 'killed_persons' column in each borough. This will help to identify any boroughs with insufficient data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows exist for each borough in the 'killed_persons' column\n",
    "borough_counts = clean_nyc1.groupby('borough')['killed_persons'].count()\n",
    "print(borough_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding Empty Boroughs from ANOVA\n",
    "\n",
    "To ensure the ANOVA test runs correctly, only boroughs with valid data in the `killed_persons` column are included.  \n",
    "Boroughs with zero non-null entries are excluded to prevent computational errors during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats \n",
    "\n",
    "# Check for how many valid rows exist for each borough and 'killed_persons' column\n",
    "borough_counts = clean_nyc1.groupby('borough')['killed_persons'].count()\n",
    "\n",
    "# Display borough counts\n",
    "print(borough_counts)\n",
    "\n",
    "# Only include boroughs with more than 0 rows of 'killed_persons'\n",
    "valid_boroughs = borough_counts[borough_counts > 0].index.tolist()\n",
    "\n",
    "# Perform ANOVA with only valid boroughs\n",
    "anova_data = [clean_nyc1[['killed_persons']][clean_nyc1['borough'] == borough] for borough in valid_boroughs]\n",
    "\n",
    "# Perform the ANOVA test\n",
    "result = stats.f_oneway(*anova_data)\n",
    "\n",
    "# Display the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing ANOVA Test\n",
    "\n",
    "After confirming that all boroughs have valid data, an ANOVA test is used to assess whether there is a statistically significant difference in the number of persons killed across boroughs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFVfMt60uGY2",
    "outputId": "f30c5a48-84d4-4104-f164-7c34ce9389d6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drops rows where the 'killed_persons' column has missing values (NaN)\n",
    "clean_nyc1.dropna(subset=['killed_persons'], how='all', inplace=True)\n",
    "\n",
    "# performs an ANOVA test to check if there's a significant difference in the number of persons killed across different boroughs\n",
    "stats.f_oneway(\n",
    "    clean_nyc1[['killed_persons']][clean_nyc1['borough'] == \"manhattan\"],\n",
    "    clean_nyc1[['killed_persons']][clean_nyc1['borough'] == \"staten island\"],\n",
    "    clean_nyc1[['killed_persons']][clean_nyc1['borough'] == \"bronx\"],\n",
    "    clean_nyc1[['killed_persons']][clean_nyc1['borough'] == \"queens\"],\n",
    "    clean_nyc1[['killed_persons']][clean_nyc1['borough'] == \"brooklyn\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oe5B8jotuGY2"
   },
   "source": [
    "Our p-value is less than 0.05, which means that it is statistically significant. There seems to be a difference between the number of persons killed by borough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-OAprtBuGY3",
    "outputId": "742a3951-c2a7-41b5-ac9e-426da24adca2"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Create dictionary of groups using correct column name\n",
    "groups = {\n",
    "    borough: clean_nyc1.loc[clean_nyc1['borough'] == borough, 'killed_motorists']\n",
    "    for borough in [\"manhattan\", \"staten island\", \"bronx\", \"queens\", \"brooklyn\"]\n",
    "}\n",
    "\n",
    "# Filter out empty groups\n",
    "non_empty_groups = [g for g in groups.values() if len(g) > 0]\n",
    "\n",
    "# Run ANOVA\n",
    "stats.f_oneway(*non_empty_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQ0jrfjSuGY3"
   },
   "source": [
    "The resulting p-value is significantly lower than 0.05, indicating a statistically significant difference in the number of motorists killed across boroughs.\n",
    "\n",
    "The null hypothesis assumed equal mean values for all boroughs. Given the result, this assumption is highly unlikely, and at least one borough differs from the others in terms of motorist fatalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GizhxjEDuGY4"
   },
   "source": [
    "Our p-value is greater than 0.05, which means that it is not statistically significant. There seems to be no difference in the mean number of cyclists killed by borough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHubdTiZuGY4",
    "outputId": "43cb28b8-0ebe-428b-cdb9-2c18610a7cff"
   },
   "outputs": [],
   "source": [
    "# performs an ANOVA test to compare the number of pedestrians killed across different boroughs\n",
    "stats.f_oneway(\n",
    "    clean_nyc1[['killed_pedestrians']][clean_nyc1['borough'] == \"manhattan\"],\n",
    "    clean_nyc1[['killed_pedestrians']][clean_nyc1['borough'] == \"staten island\"],\n",
    "    clean_nyc1[['killed_pedestrians']][clean_nyc1['borough'] == \"bronx\"],\n",
    "    clean_nyc1[['killed_pedestrians']][clean_nyc1['borough'] == \"queens\"],\n",
    "    clean_nyc1[['killed_pedestrians']][clean_nyc1['borough'] == \"brooklyn\"]  # lowercase \"brooklyn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvJ52YQVuGY3",
    "outputId": "8f3fa16a-3a1e-4537-a826-b6b71f90c27a"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# performs an ANOVA test to compare the number of cyclists killed across different boroughs\n",
    "stats.f_oneway(\n",
    "    clean_nyc1[['killed_cyclists']][clean_nyc1['borough'] == \"manhattan\"],\n",
    "    clean_nyc1[['killed_cyclists']][clean_nyc1['borough'] == \"staten island\"],\n",
    "    clean_nyc1[['killed_cyclists']][clean_nyc1['borough'] == \"bronx\"],\n",
    "    clean_nyc1[['killed_cyclists']][clean_nyc1['borough'] == \"queens\"],\n",
    "    clean_nyc1[['killed_cyclists']][clean_nyc1['borough'] == \"brooklyn\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8J9C7fOuGY4"
   },
   "source": [
    "The ANOVA test for cyclist fatalities yields a p-value greater than 0.05, indicating no statistically significant difference across boroughs.  \n",
    "Similarly, the p-value for pedestrian fatalities also exceeds 0.05, suggesting no meaningful variation between boroughs in that category.\n",
    "\n",
    "In summary, statistically significant differences exist in the number of persons and motorists killed across boroughs,  \n",
    "while the differences for pedestrians and cyclists are not significant.\n",
    "\n",
    "Let's also see whether there is a difference in the numbers of persons killed by season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bx5VMifluGY4",
    "outputId": "3a64b9cd-cbbe-49c7-cfcb-3a54a848d42c"
   },
   "outputs": [],
   "source": [
    "# displays all unique values in the 'season' column to inspect the categories of seasons\n",
    "clean_nyc1.season.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5V6qAnuSuGY5",
    "outputId": "6dc50342-8778-46d8-f4de-cab2da3233e9"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# prints column names to confirm 'killed_persons' and 'season' are present\n",
    "print(clean_nyc1.columns)\n",
    "\n",
    "# performs an ANOVA test to compare number of persons killed across different seasons\n",
    "stats.f_oneway(\n",
    "    clean_nyc1.loc[clean_nyc1['season'] == 'Autumn', 'killed_persons'],\n",
    "    clean_nyc1.loc[clean_nyc1['season'] == 'Spring', 'killed_persons'],\n",
    "    clean_nyc1.loc[clean_nyc1['season'] == 'Summer', 'killed_persons'],\n",
    "    clean_nyc1.loc[clean_nyc1['season'] == 'Winter', 'killed_persons']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MKDvWE3uGY5"
   },
   "source": [
    "The ANOVA test returned a p-value of 0.0026, which is below the standard 0.05 threshold. This result indicates a statistically significant difference in the number of persons killed across different seasons.\n",
    "While the ANOVA confirms that a difference exists, it does not identify which specific seasons differ. Further comparison tests are required to determine where the variation lies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeYy5pF8uGY5"
   },
   "source": [
    "<br><div style=\"text-align: right\">[Statistical Analysis Index](#statistical_analysis)</div>\n",
    "## Two-Sample T-Tests <a id='2ttest'></a>\n",
    "\n",
    "A two-sample t-test is used to assess whether the mean number of motorists killed differs significantly between Manhattan and Staten Island. This test assumes that the distribution of values in each group follows a normal distribution. The goal is to determine whether the observed difference between means is statistically meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WleWA3weuGY5",
    "outputId": "6ed0a5ce-d948-4f25-eea2-9240aafcfcbb"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# performs an independent t-test to compare the number of motorists killed between Manhattan and Staten Island\n",
    "stats.ttest_ind(\n",
    "    clean_nyc1[clean_nyc1['borough'] == 'manhattan']['killed_motorists'],\n",
    "    clean_nyc1[clean_nyc1['borough'] == 'staten island']['killed_motorists'],\n",
    "    equal_var=False  # safer when sample sizes or variances differ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfwxvOzXuGY5"
   },
   "source": [
    "The resulting statistic represents the standardized distance between the two group means. The p-value is extremely small, indicating a statistically significant difference between Manhattan and Staten Island."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFqGuDv7uGY6",
    "outputId": "6cfcb2f6-b409-4291-c194-9fd09f3fcf90"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# performs an independent t-test to compare the number of motorists killed between Brooklyn and Staten Island\n",
    "stats.ttest_ind(\n",
    "    clean_nyc1[clean_nyc1['borough'] == 'brooklyn']['killed_motorists'],\n",
    "    clean_nyc1[clean_nyc1['borough'] == 'staten island']['killed_motorists'],\n",
    "    equal_var=False  # use this for unequal group sizes or variances\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtDScAZVuGY6"
   },
   "source": [
    "Since the p-value is less than 0.05, the result is statistically significant. The null hypothesis is that the mean number of motorists killed is the same in both boroughs â€” can be rejected.  \n",
    "A comparison between Queens and Manhattan is now examined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hobMEnequGY6",
    "outputId": "76c2607e-89c2-4292-98a2-419a00af29f4"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# performs an independent t-test to compare the number of motorists killed between Queens and Manhattan\n",
    "stats.ttest_ind(\n",
    "    clean_nyc1[clean_nyc1['borough'] == 'queens']['killed_motorists'],\n",
    "    clean_nyc1[clean_nyc1['borough'] == 'manhattan']['killed_motorists'],\n",
    "    equal_var=False  # safer when sample sizes or variances differ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MKP5wHGuGY7"
   },
   "source": [
    "This time, the p-value is even smaller, confirming a statistically significant difference between Queens and Manhattan.\n",
    "Although there is a measurable difference, it is important to assess the practical significance.  To do this, Cohenâ€™s d is used to calculate the effect size between groups.  \n",
    "\n",
    "The first comparison examines motorists killed in Manhattan versus Queens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z19tnujOuGY7"
   },
   "outputs": [],
   "source": [
    "from numpy import var, mean\n",
    "from math import sqrt\n",
    "\n",
    "# defines a function to calculate Cohen's d, which measures the effect size between two groups\n",
    "def cohens_d(first, second):\n",
    "    # calculates variance for both groups (using ddof=1 for sample variance)\n",
    "    s1, s2 = var(first, ddof=1), var(second, ddof=1)\n",
    "    # calculates pooled standard deviation\n",
    "    std = sqrt(((len(first) - 1) * s1 + (len(second) - 1) * s2) / (len(first) + len(second) - 2))\n",
    "    # returns Cohen's d effect size (mean difference divided by pooled standard deviation)\n",
    "    return (mean(first) - mean(second)) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAPndlAsuGY8",
    "outputId": "065e720d-00b5-4fab-a347-d119ac671d84"
   },
   "outputs": [],
   "source": [
    "# defines the cohens_d function if not already defined, which calculates the effect size between two groups\n",
    "def cohens_d(x, y):\n",
    "    nx = len(x)\n",
    "    ny = len(y)\n",
    "    dof = nx + ny - 2  # degrees of freedom for two independent samples\n",
    "    return (x.mean() - y.mean()) / (((nx - 1)*x.var() + (ny - 1)*y.var()) / dof)**0.5\n",
    "\n",
    "# uses the cohens_d function to calculate the effect size between motorists killed in Manhattan and Queens\n",
    "cohens_d(\n",
    "    clean_nyc1[clean_nyc1['borough'] == \"manhattan\"]['killed_motorists'],\n",
    "    clean_nyc1[clean_nyc1['borough'] == \"queens\"]['killed_motorists']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJfrKqRTuGY8"
   },
   "source": [
    "The resulting effect size is very small, suggesting that the practical difference between Manhattan and Queens is minimal.  \n",
    "Next, the effect size is evaluated for Manhattan versus Staten Island."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCEez59ruGY8",
    "outputId": "6c86db34-0a43-4094-f89d-842f76b23778"
   },
   "outputs": [],
   "source": [
    "# calculates the effect size (Cohen's d) between the number of motorists killed in Manhattan and Staten Island\n",
    "cohens_d(\n",
    "    clean_nyc1[clean_nyc1['borough'] == \"manhattan\"]['killed_motorists'],\n",
    "    clean_nyc1[clean_nyc1['borough'] == \"staten island\"]['killed_motorists']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOnzazohuGY8"
   },
   "source": [
    "Again, the effect size is very small. Although a statistical difference may exist, the actual magnitude of the difference between Manhattan and Staten Island is negligible.\n",
    "\n",
    "Next, the analysis examines whether there are statistically significant differences in the number of fatalities associated with different vehicle types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dunvce0ouGY8",
    "outputId": "f9a4a965-24a0-43c8-e05e-684072d38d5c"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# performs an independent t-test to compare the number of persons killed in SUV vs Sedan accidents\n",
    "stats.ttest_ind(\n",
    "    clean_nyc1[clean_nyc1['vehicle_type_1'] == \"suv\"]['killed_persons'],\n",
    "    clean_nyc1[clean_nyc1['vehicle_type_1'] == \"sedan\"]['killed_persons']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LU7gN1_uuGY8"
   },
   "source": [
    "The p-value is 0.42, which is well above the standard 0.05 threshold. This indicates that there is no statistically significant difference in the number of persons killed between SUV and sedan crashes.  \n",
    "While the mean fatality rate may differ slightly, this difference is not strong enough to be considered meaningful in this analysis.\n",
    "\n",
    "\n",
    "An additional question is whether SUVs pose a greater danger specifically to pedestrians, given their larger size and height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7iR8ae1uGY9",
    "outputId": "54adf241-7abb-42c7-e85f-3ebd0f42f7d2"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# performs an independent t-test to compare the number of pedestrians killed in SUV vs Sedan accidents\n",
    "stats.ttest_ind(\n",
    "    clean_nyc1[clean_nyc1['vehicle_type_1'] == \"suv\"]['killed_pedestrians'],\n",
    "    clean_nyc1[clean_nyc1['vehicle_type_1'] == \"sedan\"]['killed_pedestrians']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynrO9He_uGY9"
   },
   "source": [
    "The p-value is extremely small, indicating a highly significant difference in pedestrian fatalities between SUVs and sedans. This suggests that pedestrians are at greater risk when involved in crashes with SUVs compared to sedans.\n",
    "\n",
    "Let's see if SUVs are significantly different to the largest/broadest set of fatal vehicle types: \"passenger vehicle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGymhDqUuGY9",
    "outputId": "3d4c5c5f-70b9-4bb3-9357-c7fa4543356b"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# performs an independent t-test to compare the number of persons injured in SUV vs Passenger Vehicle accidents\n",
    "stats.ttest_ind(\n",
    "    clean_nyc1[clean_nyc1['vehicle_type_1'] == \"suv\"]['injured_persons'],\n",
    "    clean_nyc1[clean_nyc1['vehicle_type_1'] == \"passenger vehicle\"]['injured_persons']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ll7hF4bFuGY9"
   },
   "source": [
    "The comparison between SUVs and passenger vehicles for overall injuries returned a `NaN` result. This indicates a possible issue in the data, such as missing or undefined values, preventing a valid statistical conclusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2khpKO_IuGY-",
    "outputId": "f4f3d861-052e-4de8-824a-eca009d7625a"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# performs an independent t-test to compare the number of pedestrians killed in SUV vs Passenger Vehicle accidents\n",
    "stats.ttest_ind(\n",
    "    clean_nyc1[clean_nyc1['vehicle_type_1'] == \"suv\"]['killed_pedestrians'],\n",
    "    clean_nyc1[clean_nyc1['vehicle_type_1'] == \"passenger vehicle\"]['killed_pedestrians']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_APFDMXxuGY-"
   },
   "source": [
    "There is a statistically significant difference in pedestrian fatalities between SUVs and passenger vehicles.\n",
    "\n",
    "SUVs pose a higher risk to pedestrians in NYC, supported by the low p-value (0.0028)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bczYrC2kuGY-"
   },
   "source": [
    "<br><div style=\"text-align: right\">[Statistical Analysis Index](#statistical_analysis)</div>\n",
    "\n",
    "## Chi-Square <a id='chi-sq'></a>\n",
    "\n",
    "Chi-square, or chi-squared, is a measure of independence of categorical variables. Here there will be categories like borough and season. Chi-square requires frequency counts or a contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIp0MFOtuGY_",
    "outputId": "53de1ef3-a1fd-401c-8632-e21fdecbd3b4"
   },
   "outputs": [],
   "source": [
    "# creates a contingency table to show the frequency of accidents across boroughs and seasons\n",
    "contingency_table = pd.crosstab(\n",
    "    clean_nyc1['borough'], clean_nyc1['season'],\n",
    "    margins=True  # adds a total count for each row and column\n",
    ")\n",
    "\n",
    "# displays the contingency table\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYn9WwN0uGY_",
    "outputId": "7b75f847-d6e4-471f-a890-80c3e700b1df"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# performs a chi-squared test to check if there's a significant association between boroughs and seasons\n",
    "stats.chi2_contingency(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1sl6CQquGY_"
   },
   "source": [
    "The Chi-square statistic is 282.1 and the p-value is extremely small, indicating statistical significance. This suggests that season and borough are not independentâ€”season appears to influence the number of accidents in each borough.\n",
    "<br><div style=\"text-align: right\">[Statistical Analysis Index](#statistical_analysis)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5MhAHMDuGY_"
   },
   "source": [
    "Now onto the visualisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30LnGp5YuGZA"
   },
   "source": [
    "# Visualizing the data <a id='Visualizations'></a>\n",
    "We will create an exploratory visualization of:\n",
    "\n",
    "* [Number of Deaths by Borough](#Fataities_by_borough)\n",
    "* [Number of Crashes by Hour](#Crashes_by_hour)\n",
    "* [Number of Crashes by Season](#accidents_by_season)\n",
    "* [Fatal Vehicle Crash Locations](#vehicle_crash_locations)\n",
    "* [Contributing Factors to Crash Fatalities](#Factor_Bar_Plot)\n",
    "* [Fatalities to Pedestrians vs Cyclists vs Motorists](#Fataity_Grouped_Series)\n",
    "* [Fatal Crash Frequency Over Time](#Fatality_Time_Scatterplot)\n",
    "* [Crash Factor Percentages in Queens](#queens_crash_causes)\n",
    "<br><div style=\"text-align: right\">[Begining of the page](#Top)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyw8NkPauGZA"
   },
   "source": [
    "## Number of Deaths by Borough <a id='Fataities_by_borough'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USXxdKDDuGZA"
   },
   "source": [
    "What is the sum of killed persons by borough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wA2ySaSOuGZA",
    "outputId": "96451d77-7aa3-4e90-eb81-42f84b76d7bc"
   },
   "outputs": [],
   "source": [
    "# groups the dataset by borough and calculates the total number of persons killed in each borough\n",
    "clean_nyc1.groupby('borough')['killed_persons'].agg(['sum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot: Distribution of Persons Killed Across Boroughs\n",
    "This plot shows how the number of persons killed varies within each borough. Boxplots help visualize the spread and identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='borough', y='killed_persons', data=clean_nyc1)\n",
    "plt.title('Distribution of Persons Killed by Borough')\n",
    "plt.xlabel('Borough')\n",
    "plt.ylabel('Number of Persons Killed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot highlights differences in the range and central tendency of fatalities between boroughs. Boroughs like Brooklyn and Queens may show wider distributions compared to others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violinplot: Seasonal Variation in Fatalities\n",
    "This violin plot displays how the number of persons killed varies across different seasons. It combines boxplot and kernel density estimation to show the distribution shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='season', y='killed_persons', data=clean_nyc1, inner='box')\n",
    "plt.title('Distribution of Persons Killed Across Seasons')\n",
    "plt.xlabel('Season')\n",
    "plt.ylabel('Number of Persons Killed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot reveals whether certain seasons have higher or more varied fatal crash rates. Spreads and peaks may suggest seasonal risk patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swarmplot: Deaths Distribution by Borough\n",
    "This swarmplot shows each individual data point for fatalities across boroughs. It helps highlight data density and clustering that may not appear in a boxplot or violinplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.stripplot(x='borough', y='killed_persons', data=clean_nyc1, size=3, jitter=True, alpha=0.5)\n",
    "plt.title('Individual Fatalities by Borough')\n",
    "plt.xlabel('Borough')\n",
    "plt.ylabel('Number of Persons Killed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart allows us to see how densely fatalities are concentrated in certain boroughs, revealing patterns and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram: Distribution of Fatalities per Crash\n",
    "\n",
    "This histogram visualizes how many people are killed per crash incident. It helps identify whether most crashes result in minor, moderate, or major fatalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# column is numeric and filter sensibly\n",
    "clean_nyc1['killed_persons'] = pd.to_numeric(clean_nyc1['killed_persons'], errors='coerce')\n",
    "filtered = clean_nyc1[(clean_nyc1['killed_persons'] >= 0) & (clean_nyc1['killed_persons'] <= 3)]\n",
    "\n",
    "# lot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=filtered, x='killed_persons', bins=4, discrete=True, color='crimson')\n",
    "\n",
    "plt.title('Distribution of Fatalities per Crash', fontsize=15)\n",
    "plt.xlabel('Number of Persons Killed')\n",
    "plt.ylabel('Crash Count')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmap of Key Numeric Variables\n",
    "\n",
    "This heatmap visualizes the pairwise correlations between important numerical variables in the dataset, such as injuries, fatalities, and the hour of the crash. Darker shades indicate stronger relationships (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select relevant numerical columns\n",
    "num_cols = clean_nyc1[['injured_persons', 'killed_persons', 'injured_pedestrians', 'killed_pedestrians',\n",
    "                       'injured_cyclists', 'killed_cyclists', 'injured_motorists', 'killed_motorists', 'hour']]\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr = num_cols.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Correlation Heatmap of Injuries, Fatalities, and Crash Time\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssu8sxfruGZB"
   },
   "source": [
    "## Number of Crashes by Hour  <a id='Crashes_by_hour'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwMxRbFOuGZC"
   },
   "source": [
    "This section explores the mean of crashes by hour. The goal is to observe the percentage rate of crashes for each hour. Viewing this information hourly provides a better general overview. The `persons_killed` column is used, and it counts all rows regardless of deaths or injuries, enabling analysis of the number of crashes by hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bST3uyn3uGZC",
    "outputId": "248896f1-e3b5-4c9d-fcf8-2b53cae00329"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# generates a plot showing the number of crashes by hour of the day\n",
    "clean_nyc1.groupby(clean_nyc1['hour'])['killed_persons'].count().plot(figsize=(10, 6))\n",
    "plt.title('Number of Crashes by Hour', fontsize=18)  # sets the title of the plot\n",
    "plt.ylabel('Number of Crashes', fontsize=13)  # labels the y-axis\n",
    "plt.xticks(np.arange(0, 24, step=3))  # sets the x-axis labels to show hours at 3-hour intervals\n",
    "plt.xlabel('Time', fontsize=13)  # labels the x-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afEh4sWouGZC"
   },
   "source": [
    "### Fatal Crashes by Time of Day\n",
    "\n",
    "\n",
    "\n",
    "This section explores the relationship between the number of fatalities in a crash and the time of day.  \n",
    "\n",
    "Most fatal crashes involving **multiple deaths** tend to happen during late-night or early-morning hours (22:00 to 04:00), when visibility is lower and roads may be more dangerous.\n",
    "\n",
    "\n",
    "\n",
    "The scatter plot below visualises the distribution of fatal crashes across 24 hours of the day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EdfaghQuGZD"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# creates a scatter plot to visualize the relationship between number of persons killed and hour of the day\n",
    "fig = px.scatter(\n",
    "    clean_nyc1,\n",
    "    x='killed_persons',   # Correct column name for the number of fatalities\n",
    "    y='hour'\n",
    ")\n",
    "fig.show()  # displays the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8OhFxBLuGZD"
   },
   "source": [
    "<br><div style=\"text-align: right\">[Visualization Index](#Visualizations)</div>\n",
    "## Number of Crashes by Season <a id='accidents_by_season'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOezjrisuGZD",
    "outputId": "91ccfb67-e9b1-4e54-de5a-7d307091b90f"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# creates a count plot to visualise the distribution of accidents by season, with each season represented in different colors\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax = sns.countplot(x='season', hue='season', data=clean_nyc1, palette='Set3', legend=False)  # removes the legend for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBpFNZtMuGZE"
   },
   "source": [
    "Autumn and Summer have higher number of accidents compared to Spring and Winter.\n",
    "<br><div style=\"text-align: right\">[Visualization Index](#Visualizations)</div>\n",
    "## Fatal Vehicle Crash Locations <a id='vehicle_crash_locations'></a>\n",
    "To work on this section a new dataframe should be prepared with the latitude and longitude values. It will also include other columns that may be displayed in the visualization in some way, shape, or form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NU3bpwHjuGZE",
    "outputId": "361ebfc6-156a-4ca9-a074-9f8bc2c4c742"
   },
   "outputs": [],
   "source": [
    "# creates a new dataframe with selected columns related to crash locations and fatalities for further analysis\n",
    "crash_locations = clean_nyc1[['borough', 'date', 'lat', 'lon',\n",
    "                              'killed_persons', \n",
    "                              'killed_cyclists', \n",
    "                              'killed_pedestrians', \n",
    "                              'killed_motorists']].copy()\n",
    "\n",
    "# displays the first five rows of the new dataframe\n",
    "crash_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4panP4I_uGZE"
   },
   "source": [
    "The first thing that will be done is remove any non-fatal rows. To do so, first the `persons_killed` column values should be changed from float to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqW5RcDluGZE",
    "outputId": "7405247e-5ced-42a1-e183-563f689652a7"
   },
   "outputs": [],
   "source": [
    "# drops rows where the killed_persons column has missing values (NaN)\n",
    "crash_locations.dropna(subset=['killed_persons'], how='all', inplace=True)\n",
    "\n",
    "# converts the killed_persons column to integer type for consistency\n",
    "crash_locations['killed_persons'] = crash_locations['killed_persons'].astype(int)\n",
    "\n",
    "# displays the first five rows of the updated dataframe\n",
    "crash_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeHbVb9cuGZF"
   },
   "source": [
    "Much better. Now to drop the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDGRWnvBuGZL",
    "outputId": "4605adf0-f198-4c90-bb89-ebfad8d1b15b"
   },
   "outputs": [],
   "source": [
    "# filters out rows where the number_of_persons_killed column is zero and resets the index for the cleaned dataframe\n",
    "crash_locations = crash_locations[crash_locations['killed_persons'] != 0].reset_index(drop=True)\n",
    "\n",
    "# displays the first five rows of the filtered dataframe\n",
    "crash_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvy8Lxy3uGZM"
   },
   "source": [
    "Check if can be cleaned further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmyMhW43uGZM",
    "outputId": "9c08529a-6b6e-4d4f-8c3f-f0b67d6daddd"
   },
   "outputs": [],
   "source": [
    "# displays the shape of the crash_locations dataframe (number of rows and columns)\n",
    "crash_locations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qti6EynxuGZN",
    "outputId": "bb9c8a75-6227-4056-d140-f8a4d7d120b1"
   },
   "outputs": [],
   "source": [
    "# generates summary statistics for the crash_locations dataframe, including count, mean, std, min, max, etc.\n",
    "crash_locations.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gyE0XQGuGZO"
   },
   "source": [
    "It looks like there are 1127 rows, and there are missing lat/long values. For this purposes, those values will be dropped. There may be other ways to recover the data, or an approximation thereof, but it is beyond the scope of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DmH4jZwuGZO",
    "outputId": "c264b0ab-f463-4e7a-a2be-e9f6645d9233"
   },
   "outputs": [],
   "source": [
    "# drops rows where the latitude column has missing values (NaN)\n",
    "crash_locations.dropna(subset=['lat'], how='all', inplace=True)\n",
    "\n",
    "# drops rows where the longitude column has missing values (NaN)\n",
    "crash_locations.dropna(subset=['lon'], how='all', inplace=True)\n",
    "\n",
    "# generates summary statistics for the crash_locations dataframe after dropping rows with missing latitude and longitude\n",
    "crash_locations.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2H-j9W0uGZO"
   },
   "source": [
    "Now onto mapping these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5pH3fHxuGZO"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# creates a scatter mapbox plot to visualize fatal vehicle crash locations\n",
    "fig = px.scatter_mapbox(\n",
    "    crash_locations,\n",
    "    lat='lat',\n",
    "    lon='lon',\n",
    "    size='killed_persons',\n",
    "    hover_name='killed_persons',\n",
    "    hover_data=[\n",
    "        \"killed_pedestrians\", \n",
    "        \"killed_cyclists\", \n",
    "        \"killed_motorists\"\n",
    "    ],\n",
    "    color='killed_persons',\n",
    "    range_color=[1, 8],\n",
    "    opacity=0.5,\n",
    "    width=850,\n",
    "    zoom=9,\n",
    ")\n",
    "\n",
    "# Use open-street-map style to avoid needing a Mapbox token\n",
    "fig.update_layout(\n",
    "    title='Fatal Vehicle Crash Locations',\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    mapbox_center_lon=-73.95  # NYC longitude (was wrongly set to 286)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZtudnWvuGZP"
   },
   "source": [
    "This looks pretty scary! These deaths represent pedestrians, cyclists, and motorists. It is hard to find roads where people have not been killed by cars in NYC!\n",
    "\n",
    "Though it must be remembered that this map represents deaths over nearly a decade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vX8VfqXZuGZP"
   },
   "source": [
    "<br><div style=\"text-align: right\">[Visualization Index](#Visualizations)</div>\n",
    "## Contributing Factors to Crash Fatalities <a id='Factor_Bar_Plot'></a>\n",
    "'value_counts' function will be used to see how frequently contributing factor values appear in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81DpSpz2uGZP",
    "outputId": "50581b0d-db89-43be-c869-17d09dd740b3"
   },
   "outputs": [],
   "source": [
    "# counts the occurrences of each contributing factor in the factor_1 column\n",
    "clean_nyc1['factor_1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHUbIdzyuGZP"
   },
   "source": [
    "For now, it will just be analysis of the first contributing factor to get a general sense of the data. firstly, looking into the causes of fatal crashes. Begun by creating a new dataframe that groups those causes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2k6lT58EuGZP",
    "outputId": "1b3c70cf-37b1-4d31-82c1-15ba19337f26"
   },
   "outputs": [],
   "source": [
    "# groups the dataset by contributing factor, sums the number of fatalities, and sorts the results\n",
    "counted_lethal_crash_factors = (\n",
    "    clean_nyc1\n",
    "    .groupby('factor_1')['killed_persons']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .sort_values(\"killed_persons\", ascending=False)\n",
    ")\n",
    "\n",
    "# displays the first five rows\n",
    "counted_lethal_crash_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15l6pBeuuGZP"
   },
   "source": [
    "Finally, dropping 0's if there are any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwF7qQTTuGZP",
    "outputId": "173e0b02-429d-48fc-e9a0-de6e0c05f6a7"
   },
   "outputs": [],
   "source": [
    "# filters out rows where the number_of_persons_killed is zero (no fatalities)\n",
    "counted_lethal_crash_factors = counted_lethal_crash_factors[\n",
    "    counted_lethal_crash_factors['killed_persons'] != 0\n",
    "]\n",
    "\n",
    "# displays the last five rows of the filtered dataset\n",
    "counted_lethal_crash_factors.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raDVoUSAuGZQ"
   },
   "source": [
    "Great. Now onto visualizing fatal crash causes in NYC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxR6nztouGZQ",
    "outputId": "00e89107-91af-4187-8572-bfe3f7ba78bc"
   },
   "outputs": [],
   "source": [
    "# creates a horizontal bar plot to visualize contributing factors by the number of persons killed\n",
    "counted_lethal_crash_factors.plot.barh(\n",
    "    x=\"factor_1\",\n",
    "    y=\"killed_persons\",\n",
    "    figsize=(10, 20)\n",
    ").invert_yaxis()  # inverts the y-axis to display the largest values at the top\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN693UTfuGZQ"
   },
   "source": [
    "This horizontal bar chart gives a slightly better sense of the data, that fatal accidents are mostly caused by traffic recklessness.\n",
    "\n",
    "This is of course only looking at fatal accidents, only at the first recorded cause, and the reports are likely not 100% reliable. However, if NYC's government wanted to do more to reduce fatalities, this is useful information.\n",
    "<br><div style=\"text-align: right\">[Visualization Index](#Visualizations)</div>\n",
    "## Fatalities to Pedestrians vs Cyclists vs Motorists <a id='Fataity_Grouped_Series'></a>\n",
    "Plotting the dataframes created earlier using Seaborn , a library built to handle python data visualizations, is possible.\n",
    "\n",
    "For this graph, the data wll overlay the rolling sum of fatalities (grouped by travel type) on the same graph, to get a sense of relative danger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yKWC8w0uGZQ",
    "outputId": "35463031-58b3-4f71-9003-00b9fb3a520e"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# creates a figure with a specific size\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# plots the cumulative sum of motorists killed over time in red\n",
    "sns.lineplot(x='date', y='sum', color='red', data=killed_motorists, ax=ax)\n",
    "\n",
    "# plots the cumulative sum of cyclists killed over time in blue\n",
    "sns.lineplot(x='date', y='sum', color='blue', data=killed_cyclist, ax=ax)\n",
    "\n",
    "# plots the cumulative sum of pedestrians killed over time in green\n",
    "sns.lineplot(x='date', y='sum', color='green', data=killed_pedestrians, ax=ax)\n",
    "\n",
    "# adds a legend for the three groups\n",
    "ax.legend(['Motorists', 'Cyclists', 'Pedestrians'], facecolor='w')\n",
    "\n",
    "# displays the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0Vw8DYwuGZQ"
   },
   "source": [
    "This is interesting. More pedestrians are killed than motorists by vehicle crashes. Which isn't entirely surprising, but it is disheartening.\n",
    "\n",
    "This analysis lends itself well to comparing injury rates as well, which is expected to be significantly higher.\n",
    "\n",
    "Assuming there was more time, further analysis into the rates of change in the fatality set (to see if NYC is getting safer or more dangerous) would be manageable with another function over these data sets.\n",
    "<br><div style=\"text-align: right\">[Visualization Index](#Visualizations)</div>\n",
    "## Fatal Crash Frequency Over Time <a id='Fatality_Time_Scatterplot'></a>\n",
    "To visualize the data, refining the dataframe is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrdAxPfouGZQ",
    "outputId": "3095f9fd-4e4c-4bd0-b43a-824045ca2bb8"
   },
   "outputs": [],
   "source": [
    "# filters the dataset to include only rows where at least one person was killed in the crash\n",
    "lethal_crashes = clean_nyc1[clean_nyc1['killed_persons'] > 0]\n",
    "\n",
    "# displays the first five rows of the filtered dataset\n",
    "lethal_crashes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vP53L2N7uGZQ"
   },
   "source": [
    "Now to group by `crash_date`... The number of chained functions here will be used here. The number of fatalities per day will be added, and sorting chronologically.\n",
    "\n",
    "Sidenote: the time column is changed, accidentally adding today's date, but that will not have an impact on the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SN7jG4syuGZS",
    "outputId": "05555958-18e2-4d17-9508-d3df88051e6c"
   },
   "outputs": [],
   "source": [
    "# groups the dataset by date and sums the number of persons killed on each day\n",
    "clean_nyc1.groupby('date')['killed_persons'].sum().reset_index().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kfuJ-7nuGZS"
   },
   "source": [
    "Next, this will be ported into a new dataframe with some other fields that are considered for analysis, then try to use matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOAyZOaduGZT",
    "outputId": "012694e2-a908-4e9c-fd55-ac494ccd43cb"
   },
   "outputs": [],
   "source": [
    "# groups the dataset by date, borough, and contributing factor, then sums the number of persons killed for each group\n",
    "counted_lethal_crashes = clean_nyc1.groupby(['date', 'borough', 'factor_1'])['killed_persons'].sum().reset_index()\n",
    "\n",
    "# displays information about the grouped dataset, including column types and non-null counts\n",
    "counted_lethal_crashes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4EgGIBMuGZU"
   },
   "source": [
    "Let's visualize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEhnkaKpuGZU",
    "outputId": "dc62d2dc-4870-4018-afb6-9d62683e2f12"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ensures that the 'date' column is in datetime format\n",
    "clean_nyc1['date'] = pd.to_datetime(clean_nyc1['date'])\n",
    "\n",
    "# groups the dataset by month and sums the number of fatalities for each month\n",
    "monthly_deaths = clean_nyc1.groupby(clean_nyc1['date'].dt.to_period('M'))['killed_persons'].sum()\n",
    "monthly_deaths.index = monthly_deaths.index.to_timestamp()  # converts period index to timestamp for plotting\n",
    "\n",
    "# plots the monthly deaths trend\n",
    "plt.figure(figsize=(15, 5))\n",
    "monthly_deaths.plot()\n",
    "plt.title(\"Monthly Deaths in NYC Crashes\")\n",
    "plt.ylabel(\"Number of Persons Killed\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8TiJzmNuGZU"
   },
   "source": [
    "This has successfully plotted the crash fatalities in NYC over time!\n",
    "\n",
    "It can seen from this that the majority of crashes have either 0 or 1 persons killed. Though there are some outliers. This could be plugged into other columns into this chart, and see distributions of pedestrians, cyclists, motorists killed, but it will be easier and more visually appealing with pandas and seaborn. Here it is hard to get a precise view of the data, though it does help knowing where to look.\n",
    "<br><div style=\"text-align: right\">[Visualization Index](#Visualizations)</div>\n",
    "## Crash Factor Percentages in Queens <a id='queens_crash_causes'></a>\n",
    "Checking out the data for boroughs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcRVOjMfuGZU",
    "outputId": "9fd58317-c64b-44fe-998e-cbaeb2d350e7"
   },
   "outputs": [],
   "source": [
    "# counts the occurrences of each borough in the dataset to identify the top boroughs\n",
    "top_boroughs = clean_nyc1['borough'].value_counts()\n",
    "\n",
    "# displays the top boroughs based on the number of accidents\n",
    "top_boroughs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAVEfYj9uGZV"
   },
   "source": [
    "Now onto investigating the crashes to see which factor contributed first to the crash in each borough. These columns are selected and put into a dataframe to look at the information.\n",
    "\n",
    "The borough of Queens is selected to analyse the neighbourhood for crash data. So, only this variable is selected from the borough column and makes a dataframe for factor contributing to vehicle crash for neighborhood Queens. This comparison is for the first factor involved in the crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hDKZNpOuGZV",
    "outputId": "aeafe7cb-f070-428a-f0b8-11d249f51775"
   },
   "outputs": [],
   "source": [
    "# selects the 'borough' and 'factor_1' columns for further analysis\n",
    "borough_factor = clean_nyc1.loc[:, ['borough', 'factor_1']]\n",
    "\n",
    "# filters the data to include only rows where the borough is 'queens'\n",
    "queens_data = borough_factor[borough_factor.borough == 'queens']\n",
    "\n",
    "# displays the first five rows of the filtered data for Queens\n",
    "queens_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4_W-j6MuGZW"
   },
   "source": [
    "For this data, what are the numbers for each contributing factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVUxGKKRuGZW",
    "outputId": "a6054340-9373-4165-f642-d1a0bc7b775e"
   },
   "outputs": [],
   "source": [
    "# counts the occurrences of each contributing factor in accidents for the borough of Queens\n",
    "reason_counts = queens_data['factor_1'].value_counts()\n",
    "\n",
    "# displays the count of each contributing factor in Queens accidents\n",
    "reason_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_Zu2HmfuGZX"
   },
   "source": [
    "Now onto finding the percentage contribution of each factor for the borough of Queens and the first contributing factor involved in the crash to determine which factor had the greatest impact for this group. This will be expressed as a percentage of the total number of contributing factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_inq68UkuGZX",
    "outputId": "d453614d-a146-452b-ad79-c3181c79e8a3"
   },
   "outputs": [],
   "source": [
    "total_reason_counts = reason_counts.sum()\n",
    "total_reason_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxCqQFAOuGZX",
    "outputId": "1629135d-f98d-48ef-a881-a5d2918215ad"
   },
   "outputs": [],
   "source": [
    "reason_counts1 = reason_counts/total_reason_counts*100\n",
    "reason_counts1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByGi7vUVuGZX"
   },
   "source": [
    "The greatest impact to crash was traffic recklessness, which is followed by driver inattention/distraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Contributing Factors to Crashes in Queens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ntc9wZ6AuGZX",
    "outputId": "3883c40a-472a-4191-94cf-28045e388bfd"
   },
   "outputs": [],
   "source": [
    "# creates a horizontal bar plot to visualize the distribution of contributing factors in crashes in Queens\n",
    "reason_counts.plot.barh(figsize=(10, 20)).invert_yaxis()  # inverts the y-axis to display the largest values at the top\n",
    "plt.title('Percentage of Factors Leading to Crashes in Queens', fontsize=15)  # adds a title to the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model: Predicting Fatal Crashes\n",
    "\n",
    "In this final section, there will be an implementation of a simple machine learning model to predict whether a motor vehicle crash is fatal or not. This adds a predictive layer on top of the previous statistical analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a binary column 'fatal_crash': 1 if killed_persons > 0, else 0\n",
    "clean_nyc1['fatal_crash'] = (clean_nyc1['killed_persons'] > 0).astype(int)\n",
    "\n",
    "#preview to confirm\n",
    "clean_nyc1[['killed_persons', 'fatal_crash']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Features for Classification\n",
    "\n",
    "Now, selecting relevant features that may help predict whether a crash is fatal. Features like the borough, time of crash, vehicle type, and contributing factors could be influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use correct column names based on your dataset\n",
    "features = ['borough', 'hour', 'vehicle_type_1', 'factor_1']\n",
    "model_df = clean_nyc1[features + ['fatal_crash']].dropna()\n",
    "\n",
    "#check class balance (how many fatal vs non-fatal)\n",
    "model_df['fatal_crash'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#define X (features) and y (target)\n",
    "X = model_df[features]\n",
    "y = model_df['fatal_crash']\n",
    "\n",
    "# split the data into training and testing sets (stratify to preserve class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# define preprocessing for categorical features using one-hot encoding\n",
    "categorical_features = ['borough', 'vehicle_type_1', 'factor_1']\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # keep 'hour' as-is\n",
    ")\n",
    "\n",
    "# create a pipeline with preprocessing and a logistic regression model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# show evaluation\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate features (X) and label (y)\n",
    "X = model_df.drop('fatal_crash', axis=1)\n",
    "y = model_df['fatal_crash']\n",
    "\n",
    "# split data 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# confirm the split\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Records Before and After Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = datanyc.dropna(subset=['LATITUDE', 'LONGITUDE', 'BOROUGH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before:\", len(datanyc))\n",
    "print(\"After:\", len(cleaned_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleaning comparison chart\n",
    "before_cleaning = 2170974\n",
    "after_cleaning = 1462699\n",
    "\n",
    "counts = [before_cleaning, after_cleaning]\n",
    "labels = [\"Before Cleaning\", \"After Cleaning\"]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(labels, counts, color=['lightcoral', 'mediumseagreen'])\n",
    "plt.title(\"Number of Records Before and After Data Cleaning\")\n",
    "plt.ylabel(\"Number of Rows\")\n",
    "plt.tight_layout()\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval + 10000, f'{yval:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Balancing with SMOTE\n",
    "\n",
    "To address the severe class imbalance in the `fatal_crash` target variable, SMOTE (Synthetic Minority Over-sampling Technique) was applied. SMOTE generates synthetic examples for the minority class to improve model performance and prevent bias toward the majority class. After applying SMOTE, both classes are equally represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "\n",
    "# features and target\n",
    "X = model_df.drop(columns='fatal_crash')\n",
    "y = model_df['fatal_crash']\n",
    "\n",
    "# identify categorical and numeric columns\n",
    "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "numeric = X.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# define preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', StandardScaler(), numeric)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False,\n",
    "    force_int_remainder_cols=False \n",
    ")\n",
    "\n",
    "# encode and transform features\n",
    "X_encoded = preprocessor.fit_transform(X)\n",
    "\n",
    "# apply SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_encoded, y)\n",
    "\n",
    "print(\"After SMOTE:\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline with preprocessing and a faster Random Forest model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=50,     # Fewer trees = faster\n",
    "        max_depth=10,        # Limit tree depth = faster\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "This plot helps understand which features contributed most to the model's prediction of fatal crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only works if classifier is RandomForest (skip if using another model)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get feature names from preprocessing\n",
    "feature_names = model_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "importances = model_pipeline.named_steps['classifier'].feature_importances_\n",
    "\n",
    "# create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), hue='Feature', dodge=False, palette='viridis', legend=False)\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  \n",
    "Due to the heavy class imbalance in the dataset (very few fatal crashes), the model struggled with precision. However, recall for fatal crashes improved significantly after applying class_weight='balanced'. This indicates the model is now at least identifying some of the fatal crashes, which is a meaningful improvement. More advanced techniques like SMOTE or ensemble models could be explored for better predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot class balance using your actual labels (replace y_train with your label variable)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.countplot(x=y_train, palette='pastel')\n",
    "plt.title(\"Target Variable Distribution\")\n",
    "plt.xlabel(\"Crash Outcome (0 = Non-Fatal, 1 = Fatal)\")\n",
    "plt.ylabel(\"Number of Records\")\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Random Forest Classifier\n",
    "\n",
    "Now that the Random Forest model has been trained, it's time to evaluate its performance using the test set. A confusion matrix will be used to see how well it predicts fatal vs. non-fatal crashes, and a classification report to inspect precision, recall, and F1-score.\n",
    "\n",
    "The confusion matrix shows how many fatal crashes (1s) were correctly predicted, and how many were missed or falsely predicted. This is especially important since fatal crashes are rare (highly imbalanced classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# predict on test data\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# show confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap=\"Blues\")\n",
    "plt.title(\"Random Forest - Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# classification report\n",
    "print(\"Classification Report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression Model (Baseline)\n",
    "\n",
    "To compare performance, the Logistic Regression model will be trained. Since the dataset is highly imbalanced (very few fatal crashes), `class_weight='balanced'` is enabled to give more attention to the minority class.\n",
    "\n",
    "It will examine its confusion matrix and classification report and compare it against the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create pipeline for Logistic Regression\n",
    "logreg_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=500, class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_logreg = logreg_pipeline.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_logreg, cmap=\"Purples\")\n",
    "plt.title(\"Logistic Regression - Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report (Logistic Regression):\")\n",
    "print(classification_report(y_test, y_pred_logreg, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation & Comparison\n",
    "\n",
    "The two models have been evaluated:\n",
    "\n",
    "1. `Random Forest Classifier` â€“ performed well on non-fatal crashes but failed entirely to detect fatal ones due to class imbalance.\n",
    "2.`Logistic Regression with class balancing` â€“ was able to detect over half of the fatal crashes, trading off some accuracy but providing a meaningful model for real-world prediction.\n",
    "\n",
    "> In safety-critical applications like crash analysis, it's more valuable to correctly identify `fatal incidents`, even at the cost of slightly lower overall accuracy.\n",
    "\n",
    "Based on this, `Logistic Regression` is the more effective model for predicting fatal crashes in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (Multi-Layer Perceptron) Neural Network Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert categorical features and standardize for the MLP model\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "X_train_encoded = pd.get_dummies(X_train)\n",
    "X_test_encoded = pd.get_dummies(X_test)\n",
    "\n",
    "# Align both sets to have the same columns\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Now scale the numeric values\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the MLP model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Build a simple MLP model\n",
    "mlp_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "mlp_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = mlp_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=8,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MLP Model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Predict class labels (threshold of 0.5 for binary classification)\n",
    "y_pred_mlp = (mlp_model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Show accuracy\n",
    "print(\"MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n",
    "\n",
    "# Plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_mlp, cmap=\"Blues\")\n",
    "plt.title(\"MLP - ConfusionÂ Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy/Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training accuracy and loss over epochs\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', color='teal')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy', color='orange')\n",
    "plt.title('MLP Training Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss', color='purple')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', color='crimson')\n",
    "plt.title('MLP Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (Multi-Layer Perceptron) Model\n",
    "\n",
    "To explore a deep learning approach, a simple MLP model was implemented using TensorFlow/Keras. The input features were one-hot encoded and standardized. The architecture included two hidden layers with ReLU activations and dropout regularization to prevent overfitting.\n",
    "\n",
    "- **Training Epochs**: 8  \n",
    "- **Batch Size**: 32  \n",
    "- **Activation Function**: ReLU (hidden layers), Sigmoid (output)  \n",
    "- **Loss Function**: Binary Crossentropy  \n",
    "- **Optimizer**: Adam\n",
    "\n",
    "**Performance**:\n",
    "- **Accuracy**: 99.86%\n",
    "- However, the model failed to predict the minority class (fatal crashes), likely due to class imbalance.\n",
    "\n",
    "Future improvements could involve using techniques like:\n",
    "- **Class weighting**\n",
    "- **Oversampling (SMOTE)**\n",
    "- **Focal loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace these with your real accuracy values\n",
    "accuracies = {\n",
    "    \"Logistic Regression\": 0.71,\n",
    "    \"Random Forest\": 1.00,\n",
    "    \"MLP\": 0.9987\n",
    "}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(accuracies.keys(), accuracies.values(), color=['indigo', 'teal', 'darkorange'])\n",
    "plt.ylim(0.6, 1.05)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Annotate bars with values\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f\"{yval:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Summary of Statistical Findings and Model Performance\n",
    "\n",
    "| **Test/Model**                               | **Variables Compared**                                     | **Key Outcome**                                                                 |\n",
    "|---------------------------------------------|------------------------------------------------------------|---------------------------------------------------------------------------------|\n",
    "| **ANOVA**                                    | `killed_persons` across boroughs                           | Significant difference found (`p < 0.001`) â€” fatality rates vary by borough.   |\n",
    "| **ANOVA**                                    | `killed_persons` across seasons                            | No strong seasonal difference â€” distributions mostly uniform.                  |\n",
    "| **T-test**                                   | Motorists killed: Manhattan vs. Queens                     | No significant difference (`p > 0.05`)                                          |\n",
    "| **T-test**                                   | Motorists killed: Brooklyn vs. Staten Island               | Significant difference observed                                                |\n",
    "| **T-test**                                   | Pedestrians killed: SUV vs. Sedan                          | SUV-involved crashes lead to more pedestrian deaths                            |\n",
    "| **Effect Size (Cohenâ€™s d)**                  | Manhattan vs. Queens (motorist fatalities)                 | Small effect size â€” difference exists but is subtle                            |\n",
    "| **Random Forest Classifier**                 | Predict fatal crashes from borough, hour, type, factor     | Accuracy: **1.00**, but failed to detect actual fatal crashes (overfitting)    |\n",
    "| **Logistic Regression**                      | Same features as above                                     | Accuracy: **0.73**, slight improvement in catching some fatal crashes          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Neuron vs Artificial Neuron\n",
    "\n",
    "| Biological Neuron | Artificial Neuron |\n",
    "|-------------------|--------------------|\n",
    "| Receives electrical impulses via dendrites | Receives inputs (features) as numbers |\n",
    "| Integrates signal and fires if threshold is passed | Computes weighted sum and applies activation function |\n",
    "| Axon passes signal to next neuron | Output passes to next layer in the network |\n",
    "\n",
    "ðŸ§  Below is a popular comparison image showing their structure:\n",
    "![Biological vs Artificial Neuron](bvann.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_ann():\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    layer_sizes = [3, 5, 1]  # input, hidden, output\n",
    "    layer_x = [1, 2, 3]\n",
    "\n",
    "    # Draw neurons\n",
    "    for i, n_neurons in enumerate(layer_sizes):\n",
    "        for j in range(n_neurons):\n",
    "            circle = plt.Circle((layer_x[i], j - n_neurons / 2), 0.1, fill=True, color='skyblue')\n",
    "            ax.add_patch(circle)\n",
    "            if i > 0:\n",
    "                for k in range(layer_sizes[i - 1]):\n",
    "                    ax.plot([layer_x[i - 1], layer_x[i]], \n",
    "                            [k - layer_sizes[i - 1]/2, j - n_neurons/2], \n",
    "                            'gray', linewidth=0.5)\n",
    "\n",
    "    ax.set_xlim(0.5, 3.5)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.axis('off')\n",
    "    plt.title(\"Simple Feedforward ANN with 1 Hidden Layer\")\n",
    "    plt.show()\n",
    "\n",
    "draw_ann()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "relu = np.maximum(0, x)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, relu, label='ReLU')\n",
    "plt.title('ReLU Activation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, sigmoid, label='Sigmoid', color='orange')\n",
    "plt.title('Sigmoid Activation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_backprop_diagram():\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Define layer positions and neuron count\n",
    "    layers = [3, 4, 1]\n",
    "    layer_x = [1, 3, 5]\n",
    "\n",
    "    # Store neuron positions\n",
    "    positions = {}\n",
    "\n",
    "    # Draw neurons and feedforward connections\n",
    "    for i, count in enumerate(layers):\n",
    "        layer_positions = []\n",
    "        for j in range(count):\n",
    "            y = j - count / 2\n",
    "            circle = plt.Circle((layer_x[i], y), 0.2, color='lightblue', ec='black', zorder=2)\n",
    "            ax.add_patch(circle)\n",
    "            layer_positions.append((layer_x[i], y))\n",
    "            if i > 0:\n",
    "                for prev in positions[i-1]:\n",
    "                    # Feedforward connection\n",
    "                    ax.plot([prev[0], layer_x[i]], [prev[1], y], color='steelblue', linewidth=1, zorder=1)\n",
    "        positions[i] = layer_positions\n",
    "\n",
    "    # Draw backpropagation arrows\n",
    "    for i in reversed(range(1, len(layer_x))):\n",
    "        for j, (x1, y1) in enumerate(positions[i]):\n",
    "            for k, (x0, y0) in enumerate(positions[i-1]):\n",
    "                ax.annotate(\"\",\n",
    "                            xy=(x0, y0), xytext=(x1, y1),\n",
    "                            arrowprops=dict(arrowstyle='->', color='red', linestyle='--'),\n",
    "                            zorder=0)\n",
    "\n",
    "    # Labels\n",
    "    ax.text(0.5, 0, 'Input Layer', fontsize=12)\n",
    "    ax.text(2.5, 1.8, 'Hidden Layer', fontsize=12)\n",
    "    ax.text(5.5, 0, 'Output Layer', fontsize=12)\n",
    "    ax.text(6.3, 0.2, 'Error flows â†', fontsize=10, color='red')\n",
    "    ax.text(0.7, 1.7, 'â†’ Feedforward', fontsize=10, color='steelblue')\n",
    "\n",
    "    ax.set_xlim(0, 7)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.axis('off')\n",
    "    plt.title(\"Backpropagation Diagram: Forward & Error Flow\")\n",
    "    plt.show()\n",
    "\n",
    "draw_backprop_diagram()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lHI0T9UuGZX"
   },
   "source": [
    "<br><div style=\"text-align: right\">[Visualization Index](#Visualizations)</div>\n",
    "## Conclusions\n",
    "\n",
    "This initiative utilized statistical analysis, data visualization, and machine learning to identify the patterns and underlying causes of motor vehicle deaths in New York City. By converting raw vehicular crash data into meaningful insights, a number of salient points were determined.\n",
    "Key Insights:\n",
    "\n",
    "- The most common cause of fatal crashes is traffic recklessness.\n",
    "\n",
    "- SUVs contribute disproportionately to fatal pedestrian crashes in relation to passenger cars and sedans.\n",
    "- The crashes differ seasonally and on an hourly basis and are higher in the afternoons and evenings.\n",
    "- Brooklyn and Queens experience far larger volumes of crashes and fatalities compared to the other boroughs.\n",
    "- More fatal crashes involving pedestrians compared to those involving drivers pointed toward a serious safety issue.\n",
    "\n",
    "Though rare as a proportion of total crashes, fatal crashes form non-random patterns which can be predicted through data. This serves to highlight the benefits of targeted action and ongoing refinement of the practices of data collection and reporting.\n",
    "\n",
    "Well-designed data sets coupled with well-balanced predictive models hold great promise for policy direction and traffic fatality reduction.\n",
    "\n",
    "# <center> <br>[Beginning of the page](#Top)</center> <a id='Bottom'></a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
